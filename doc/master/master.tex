\documentclass[a4paper, 12pt, BCOR10mm, DIV12, toc=bibliography, toc=listof, german]{scrbook}

% Preamble {{{
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage[%
	colorlinks=false,
	pdfborder={0 0 0},
]{hyperref}
\usepackage{epstopdf}
\usepackage[numbers,sort]{natbib}
\usepackage{setspace}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{scrpage2}
\usepackage{units}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}

\definecolor{uniblue}{rgb}{0.062745,0.17647,0.34118}
\definecolor{tblue}{HTML}{204A87}
\definecolor{tgreen}{HTML}{4E9A06}
\definecolor{tred}{HTML}{CC0000}

\lstset{
	language=C,
	numbers=left,
	frame=single,
	basicstyle=\footnotesize,
	showstringspaces=false,
	captionpos=b,
	breaklines=true,
	breakatwhitespace=false,
	keywordstyle=\color{tgreen},
	commentstyle=\color{tblue},
	stringstyle=\color{tred}
}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

% Titlepage {{{
\usepackage[absolute]{textpos}

\newlength{\TitleMargin}
\newlength{\TitleWidth}

\setlength{\TitleMargin}{2cm}
\setlength{\TitleWidth}{\paperwidth}
\addtolength{\TitleWidth}{-\TitleMargin}
\addtolength{\TitleWidth}{-\TitleMargin}


\newcommand{\TitleUni}{Universität Potsdam}
\newcommand{\TitleInstitut}{Mathematisch-Naturwissenschaftliche Fakultät\\Institut für Informatik}
\newcommand{\TitleTitel}{Selbst-adaptive Lastverteilung für DNS-Cluster}
\newcommand{\TitleTyp}{Masterarbeit}
\newcommand{\TitleAutor}{Sebastian Menski}
\newcommand{\TitleBetreuerText}{Betreuer}
\newcommand{\TitleBetreuer}{Prof. Dr. Bettina Schnor\\ &M.Sc. Jörg Zinke}
\newcommand{\TitleAbschlussText}{zur Erlangung des akademischen Grades\\Master of Science\\in Informatik}
\newcommand{\TitleOrt}{Potsdam}
\newcommand{\TitleDatum}{7. Juli 2012}
\hypersetup{
	pdfauthor={Sebastian Menski},
	pdftitle={\TitleTitel},
}

\renewcommand{\maketitle}{
	\thispagestyle{empty}
	\begin{textblock*}{\TitleWidth}(\TitleMargin,\TitleMargin)
		~\hfill\includegraphics[height=2.5cm]{images/uni-logo}\\[3mm]
		{\color{uniblue}\rule{\TitleWidth}{1mm}}\\[5mm]
		{
			\centering
			\sffamily\Large
			{\LARGE\TitleUni}\\[0.5\baselineskip]
			{\large\TitleInstitut}\\[5\baselineskip]
			{\Huge\TitleTitel}\\[3\baselineskip]

			{\TitleTyp}\\
			\TitleAbschlussText\\[3\baselineskip]

			\TitleAutor\\[3\baselineskip]
			\begin{tabular}{rl}
				\TitleBetreuerText: & \TitleBetreuer
			\end{tabular}\\[2\baselineskip]
			\TitleOrt, \TitleDatum\par
		}
	\end{textblock*}
	~\clearpage
}

\def \dns {Domain Name System (DNS)}

% Titlepage }}}

% Preamble }}}


\begin{document}
	 % Frontmatter {{{
	\frontmatter
	\maketitle{}
	\tableofcontents{}
	% Frontmatter }}}

	% Mainmatter {{{
	\onehalfspacing{}
	\mainmatter
	\pagestyle{scrheadings}

	\chapter{Einleitung} % {{{
	\label{cha:einleitung}

		Das \dns{} \cite{rfc1034, rfc1035} ist eine fundamentale Komponente des heutigen Internets.  Es
		ermöglicht eine Abbildung von Domain-Namen auf sogenannte IP\footnote{Das Internet Protocol (IP)
		\cite{rfc791} ist eines der wichtigsten Netzwerkprotokolle der TCP/IP-Suite
		\cite{stevens1994}. Es dient der Vermittlung von Datenpaketen in einem Netzwerk, um dies zu
		ermöglichen, erhält jeder Netzwerkteilnehmer eine eindeutige IP-Adresse.}-Adressen. Ein
		Domain-Name ist eine Zeichenkette, welche Menschen leichter merken können als eine
		Zahlenfolge.  Computersysteme hingegen arbeiten besser und schneller mit Zahlen als mit
		Zeichenketten.  Daher ist diese Abbildung eine wichtige Voraussetzung für die einfache
		und schnelle Nutzung eines Netzwerks, im Besonderen des Internets. Bei jedem Aufruf einer
		Website oder dem Verschicken einer E-Mail, bei dem der Nutzer einen Domain-Namen verwendet, wird
		eine Abfrage an einen Server im DNS gestellt. Die darauffolgende Antwort, ist die dem
		Domain-Name zugeordnete IP-Adresse. Diese IP-Adresse wird dann vom Computersystem genutzt, um
		eine Verbindung zum Ziel aufzubauen. Bereits dieser Zusammenhang verdeutlicht die Wichtigkeit
		des DNS, da ohne dieses System jeder Nutzer eines Netzwerks die benötigten IP-Adressen
		selber kennen müsste. Das ist vielleicht für ein kleines privates Netzwerk durchaus möglich, in
		Anbetracht der Größe des heutigen Internets jedoch	kaum vorstellbar. Aus diesem Grund ist ein
		stabiles System mit schnellen Antwortzeiten unerlässlich.

		Die Informationen im DNS werden dezentral gespeichert. Sie werden in einer verteilten,
		baumartigen Struktur gespeichert (siehe Abbildung \ref{fig:tree}). Dabei können verschiedene
		Server, die einzelnen Knoten des Baums und die dazugehörigen Informationen verwalten. Diese
		Server werden im Folgenden DNS-Server oder Nameserver genannt und haben die Aufgabe, Fragen an
		das DNS zu beantworten. Aus der Struktur, des in Abbildung \ref{fig:tree} dargestellten Baums, ist
		zu erkennen, dass die kritischste Infrastruktur in den ersten beiden Ebenen des Baums existiert.
		Dabei handelt es sich um den Root-Server und alle Server der Top-Level-Domains (TLDs). Diese
		beiden Ebenen stellen die am häufigsten frequentierten Nameserver des DNS bereit, denn die Baumstruktur
		des DNS bedeutet, dass ein Knoten bzw. DNS-Server nur Informationen über den von ihm ausgehenden
		Teilbaum besitzt. Somit ist verständlich, dass die größte Menge an Informationen beim Root-Server
		und den TLDs liegt.

		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{images/tree}
			\caption{Baumstruktur des \dns{}}
			\label{fig:tree}
		\end{figure}

		%Überleitung

		Ausfallsicherheit, ständige Verfügbarkeit und kurze Antwortzeiten sind Anforderungen, welche
		auch an viele andere Netzwerk-Dienste gestellt werden. Um diese Anforderungen zu erfüllen,
		gibt es das Konzept der Lastverteilung. Dabei wird eine hohe Last auf einer Ressource dadurch
		verringert, dass die Gesamtlast auf mehrere identische Ressourcen verteilt wird. Somit ist die
		Last je Ressource geringer und besser handhabbar. Dieses Konzept findet allgemein in Netzwerken
		und bei Server-Systemen Anwendung \cite{bourke2001, kopparapu2002}. Aber auch speziellere
		Anwendungsgebiete, wie die Verteilung von Last auf Webservern, sind besonders interessant für
		Websites mit hohen Lastanforderungen \cite{meplho2012}. Hierbei wird die anfallende Last auf ein
		Cluster aus Backend-Servern verteilt. Jeder der sogenannten Backend-Server stellt die
		gleiche Anwendung bereit. Die Aufgabe der Verteilung übernimmt dabei meist ein Lastverteiler,
		welcher vor den Backend-Servern in das Netzwerk eingefügt wird. Jede Anfrage wird somit durch
		den Lastverteiler geleitet, und dann von diesem an einen verfügbaren Backend-Server
		weitergegeben. Diese allgemeinen Konzepte lassen sich ebenfalls für hohe Lastsituationen im DNS
		nutzen.

		Diese Arbeit beschäftigt sich mit dem Thema der Lastverteilung von DNS-Anfragen. Dazu wird ein
		bestehendes System \cite{zinke2007,scsczile2008,zinke2012,salbnet} zur selbst-adaptiven
		Lastverteilung von TCP-Anfragen so erweitert, dass es ebenfalls UDP-Anfragen verarbeiten kann.
		Dieses System ist in dem Sinne selbst-adaptiv, weil der Lastverteiler keine eigenständigen
		Annahmen über die Last der Backend-Server trifft. Jeder Backend-Server ermittelt eigenständig
		eine Einschätzung der aktuellen Lastsituation, welche dem Lastverteiler über Credits
		mitgeteilt wird. Dabei entspricht ein Credit einer weiteren Verbindung, die von dem Server
		abgearbeitet werden kann. Somit passt sich der Lastverteiler immer eigenständig der aktuellen
		Lastsituation auf den Backend-Server an, ohne dass ein Administrator Einstellungen am
		Lastverteiler vornehmen muss.  Dadurch unterscheidet sich das System von anderen
		Lastverteilungs-Algorithmen, welche meist initial konfiguriert werden und danach kaum
		beeinflussbar sind. So kann auch eine fehlerhafte initiale Konfiguration nicht im laufenden
		Betrieb ausgebessert werden.

		Das in \cite{zinke2007,scsczile2008} beschriebene System ist für TCP\footnote{Das Transmission
		Control Protocol (TCP) \cite{rfc793} ist ein zuverlässiges, verbindungsorientiertes und
		paketvermitteltes Transportprotokoll der TCP/IP-Suite \cite{stevens1994}. Es wird für
		Verbindungen eingesetzt, welche in dem Sinne verlässlich sind, dass verlorene Datenpakete
		erneut übertragen werden. Um dies zu realisieren, ist allerdings ein gewisser Mehraufwand beim
		Verbindungsaufbau und -abbau nötig.  Bekanntestes Beispiel für TCP-Verbindungen stellen
		HTTP-Anfragen zum Abrufen und Kommunizieren mit Websites dar.}-Anfragen implementiert. Es wurde
		anhand des Apache Webservers (httpd) \cite{httpd} getestet \cite{zinke2012}. Bei dieses Tests
		wurden die Credits über ein InfiniBand-Netzwerk \cite{infiniband,zinke2007} übertragen und direkt
		in den Speicher des Lastverteilers geschrieben\footnote{InfiniBand \cite{infiniband} ist eine
		Hochgeschwindigkeits-Übertragungstechnik, welche zusätzlich Remote Direct Memory Access (RDMA)
		unterstützt. Das ermöglicht einen direkten Speicherzugriff auf einen über das Netzwerk verbunden
		Rechner.}. Durch den direkten Schreibzugriff, muss keine blockierende Kommunikation zwischen dem
		Lastverteiler und dem Backend-Server stattfinden.

		Aufgabe dieser Arbeit ist es, dass bereits für TCP implementierte Verfahren auch für UDP zu
		implementieren. Das User Datagram Protocol (UDP) \cite{stevens1994, rfc768} ist ebenfalls ein
		paketvermitteltes Transportprotokoll aus der TCP/IP-Suite, jedoch im Gegensatz zum TCP,
		verbindungslos und unzuverlässig. Das bedeutet, der Verlust eines Datenpakets während der
		Übertragung wird nicht bemerkt, und somit ist auch keine automatisch Wiederholung der Übertragung
		möglich. Diese Eigenschaft grenzt die Anwendungsfälle, in denen UDP sinnvoll einsätzbar ist,
		bereits stark ein, allerdings entfällt durch diese Unzuverlässigkeit der Mehraufwand am Anfang
		und am Ende einer zuverlässigen Verbindung, wie es zum Beispiel bei TCP der Fall ist. Da es sich
		bei DNS-Anfragen immer nur um eine Frage und eine Antwort handelt, welche schnell übertragen
		werden sollen, ist der Mehraufwand einer TCP-Verbindung nicht sinnvoll. Deswegen wird im
		Standardfall für DNS-Anfragen UDP genutzt. Das allgemeine Vorgehen ist dabei so, dass Anfragen,
		welche nicht in einem bestimmten Zeitraum beantwortet wurden, erneut gesendet werden. Diese
		Unterschiede zwischen TCP und UDP müssen bei der Erweiterung des bestehenden Systems beachtet
		werden.  Außerdem wird zur Übertragung der Credits, zwischen den Backend-Servern und dem
		Lastverteiler, eine TCP/IP-Verbindung über Ethernet genutzt. Dadurch soll verdeutlicht werden,
		dass dieses selbst-adaptive System auch über blockierende Kommunikation effizient ist.

		Die Arbeit ist in 8 Kapitel aufgeteilt. Im Kapitel \ref{cha:grundlagen} werden die Grundlagen
		dieser Arbeit erklärt. Es handelt sich dabei um das \dns{} und Lastverteilungsansätze.  Darüber
		hinaus wird eine Einordnung dieser Arbeit in die vorgestellten Konzepte vorgenommen.
		Anschließend werden im Kapitel \ref{cha:arbeiten} einige Arbeiten aufgeführt, welche sich mit
		DNS-Clustern und Lastsituationen in eben diesen befassen. Das zu erweiternde System zur
		selbst-adaptiven Lastverteilung salbnet \cite{zinke2012,salbnet} wird im Kapitel
		\ref{cha:salbnet} beschrieben.  Im \ref{cha:konzept}. Kapitel werden das Konzept der
		erarbeiteten Erweiterung und die dazugehörigen Vorüberlegungen aufgezeigt.  Im darauf folgenden
		Kapitel \ref{cha:implementierung} wird die konkrete Umsetzung des vorher dargestellten Konzepts
		beschrieben.  Es wird hierbei auf Besonderheiten und Probleme bei der Implementation
		eingegangen.  Daran anschließend werden in Kapitel \ref{cha:messungen} die Ergebnisse einer
		Funktionsmessung und eine Einschätzung der Implementation vorgestellt. Am Ende dieser Arbeit
		folgen im Kapitel \ref{cha:zusammenfassung} eine Zusammenfassung, Einschätzung und Bewertung der
		erreichten Ziele und ein Ausblick auf mögliche Erweiterungen des Konzepts.

	% chapter Einleitung }}}

	\chapter{Grundlagen} % {{{
	\label{cha:grundlagen}

		Dieses Kapitel soll eine Einführung in die Verfahren und Techniken geben, die dieser Arbeit
		zu Grunde liegen.  Dazu wird zu Beginn des Kapitels im Abschnitt \ref{sec:dns} auf die Entstehung,
		Bedeutung und Funktionsweise des \dns{} eingegangen. Außerdem werden Softwaresysteme
		vorgestellt, welche zum Betreiben eines DNS-Servers geeignet sind. Daran anschließend soll im
		Abschnitt \ref{sec:lastverteilung} ein Überblick über bestehende Konzepte und Praktiken der
		allgemeinen Lastverteilung gegeben werden, wobei auf Besonderheiten der Lastverteilung für das
		DNS eingegangen wird. Zum Abschluss des Kapitels wird diese Arbeit im Abschnitt
		\ref{sec:grundlagen-fazit} in die vorgestellten Konzepte eingeordnet.

		\section{\dns{}} % {{{
		\label{sec:dns}

		In diesem Abschnitt wird das \dns{} \cite{rfc1034,rfc1035} beschrieben. Dazu wird im ersten
		Abschnitt \ref{sub:entstehung} auf die Entstehung des DNS eingegangen und die Notwendigkeit
		eines solchen Systems dargestellt.  Anschließend folgt in Abschnitt \ref{sub:aufbau} eine
		Erläuterung des heutigen Aufbaus des DNS und seine Nutzung zur Informationsverteilung.
		Zum Schluss wird im Abschnitt \ref{sub:software} auf die bekanntesten und verbreitetsten
		Softwaresysteme eingegangen, welche aktuell zur Bereitstellung eines DNS-Servers genutzt werden.

			\subsection{Entstehung und Bedeutung} % {{{
			\label{sub:entstehung}

			Um die Entstehung des \dns{} zu verstehen, ist es entscheidend zu wissen, wie das Internet
			entstanden und aufgebaut ist.
			
			Das sogenannte ARPANET ist ein Wide Area Network (WAN) gewesen, welches in den späten
			sechziger Jahren von der Advanced Research Projects Agency (ARPA) des
			Verteidigungsministeriums der USA aufgebaut und finanziert wurde. Es hat mehrere große
			Forschungseinrichtungen in den USA verbunden und diente der Zusammenarbeit und dem Austausch
			von Daten zwischen diesen Einrichtungen. Bereits kurze Zeit nachdem die ersten Computer an
			das ARPANET angeschlossen wurden, begann die Entwicklung der TCP/IP-Suite \cite{stevens1994},
			einer Menge an Protokollen, welche den Transport von Datenpaketen über ein Netzwerk regeln
			und ermöglichen.  Die beiden bekanntesten Protokolle aus dieser Protokollfamilie sind das
			Internet Protocol (IP) \cite{rfc791} und das Transmission Control Protocol (TCP)
			\cite{rfc793}. Anfang der achtziger Jahre wurde die TCP/IP-Suite Standard auf dem ARPANET.
			Gleichzeitig führte die Entwicklung des Berkeley Software Distribution (BSD) Unix
			Betriebssystems, welches die TCP/IP-Suite zur Verfügung stellte, dazu, dass immer mehr
			Rechenzentren und Computersysteme an das ARPANET angeschlossen wurden. Das Resultat dieser Entwicklung
			war, dass aus einer geringen Anzahl von verbundenen Systemen ein großes Netzwerk
			entstand. Dieses Netzwerk entwickelte sich schließlich, mit weiteren anderen Netzwerken, zu
			dem heutigen Internet, einem Netzwerk von Netzwerken.

			Der Sinn eines Netzwerkes ist es, dass die einzelnen Teilnehmer sich kontaktieren und
			Informationen austauschen können. Um das zu ermöglichen, müssen die Netzwerkteilnehmer
			eindeutig identifizierbar sein. Dies ist mit einem Telefonnetz vergleichbar, wo jedem
			Teilnehmer eine eindeutige Kennung, die Telefonnummer, zugewiesen ist. Durch diese Rufnummer
			kann ein Kommunikationspartner mit einem anderen in Verbindung treten. Dasselbe Prinzip gilt
			auch für ein TCP/IP-Netzwerk, wobei jedes verbundene Netzwerkinterface eine eindeutige
			IP-Adresse erhält. Dies ist laut dem IPv4-Standard \cite{rfc791} eine 32-Bit-Adresse, welche
			in der Form \texttt{141.89.249.102} dargestellt wird. Im neueren IPv6-Standard \cite{rfc2460}
			handelt es sich um 128-Bit-Adressen, welche mit der Form \texttt{2a00:1450:4016:800::1010}, im
			Vergleich zu IPv4-Adressen, deutlich komplexer sind und es somit kaum noch für einen Menschen
			möglich ist, sich diese Kennung zu merken. Darüber hinaus benötigt ein Nutzer des Netzwerks
			alle IP-Adressen der Rechner, die er über das Netzwerk erreichen will. Um den Nutzern diese
			Aufgabe abzunehmen, wurde das Prinzip der \texttt{hosts}-Dateien entwickelt, welches einem
			privaten Adressbuch ähnelt. Hierbei existiert eine Datei im lokalen Dateisystem (bei unixoiden
			Systemen die Datei \texttt{/etc/hosts}), welche bekannten IP-Adressen einen beliebigen Namen
			zuweist. Ein Beispiel für eine \texttt{/etc/hosts}-Datei ist in Listing \ref{lst:hosts} zu
			sehen. In ihr sind 2 Namen für den lokalen Rechner (Zeile 6-7) und 2 Namen für andere Geräte
			im Netzwerk (Zeile 8-9) definiert. Dadurch ist es dem Nutzer möglich andere
			Netzwerk-Teilnehmer über den zugewiesenen Namen zu kontaktieren. Der Nutzer bräuchte im
			Beispiel nur noch den Namen \textit{router} verwenden, wenn er auf das Gerät mit der
			IP-Adresse \texttt{192.168.1.1} zugreifen möchte. Das Betriebssystem nutzt dann die
			\texttt{hosts}-Datei um die entsprechende IP-Adresse zu ermitteln. Ein weiterer Vorteil dieses
			Vorgehens besteht darin, dass, wenn sich die IP-Adresse eines der Geräte im Netzwerk ändert,
			ein Anpassen der \texttt{/etc/hosts}-Datei ausreicht. Der Nutzer muss sich nicht die neue
			Adresse merken und kann weiterhin, wie gewohnt, den in der \texttt{hosts}-Datei definierten
			Namen einsetzen. Für ein kleines statisches Netzwerk ist dies eine durchaus ausreichende
			Lösung. Jedoch wuchs das ARPANET relativ schnell, was dazu führte das lokale
			\texttt{hosts}-Dateien keine sinnvolle Lösung mehr waren. Der nächste Schritt war es ein
			zentrales Verzeichnis bereitzustellen. Dies wurde durch die \texttt{HOSTS.TXT} realisiert,
			welche vom Network Information Center (NIC) des Stanford Research Institute (SRI) gepflegt
			wurde. Dieses Verzeichnis wurde zentral zur Verfügung gestellt, sodass jeder Netzwerk-Nutzer
			immer die aktuelle Version erhalten und daraus seine lokale \texttt{hosts}-Datei generieren
			konnte.  Dieser zentrale Ansatz hatte den Vorteil, dass schnell auf Änderungen reagiert werden
			konnte, diese leicht zu verteilen waren und Nameskonflikte verhindert wurden.  Allerdings ist
			auch dieser Ansatz nicht für ein Netzwerk in der Größe des heutigen Internet handhabbar.  Der
			Zeitaufwand zum Instandhalten und der Netzwerkverkehr zum Verteilen wären heutzutage nicht
			mehr vertretbar. Um diese Probleme zu lösen, wurde das DNS \cite{rfc1034} entwickelt, welches
			durch einen dezentralen Ansatz die Informationen und deren Pflege verteilt.

			\lstinputlisting[float,language=sh,caption={Beispiel \texttt{/etc/hosts}-Datei mit 4 Einträgen},label=lst:hosts]{listings/hosts}

			% subsection Entstehung und Bedeutung }}}

			\subsection{Aufbau} % {{{
			\label{sub:aufbau}

			Das \dns{} ersetzte 1987 das zentrale Verzeichnis \texttt{HOSTS.TXT}. Wie bereits im Kapitel
			\ref{cha:einleitung} beschrieben, basiert es auf einem dezentralen Ansatz, bei dem die
			Informationen in einer baumartigen Struktur gespeichert werden. So ein Baum ist auszugsweise
			in Abbildung \ref{fig:dns-tree} zu sehen. Diese Struktur des Baums, also die Knoten, sind auch
			in den Domain-Namen zu erkennen. Ein Domain-Name besteht aus mehreren Subdomains, welche durch
			Punkte getrennt sind und jeweils einem Knoten im Baum entsprechen. So besteht zum Beispiel die
			Domain \texttt{cs.uni-potsdam.de} aus den Subdomains \texttt{cs}, \texttt{uni-potsdam},
			\texttt{de} und der Root-Domain, welche meist entfällt bzw. als leere Zeichenkette
			(\glqq{}\grqq{}) dargestellt wird.

			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{images/dns-tree-domains}
				\caption{Ausschnitt aus der DNS-Topologie} % TODO: schön machen
				\label{fig:dns-tree}
			\end{figure}

			Der Vorteil der Speicherung der DNS-Informationen in einem Baum besteht darin, dass jede Subdomain
			nur Informationen zu dem darunter liegenden Teilbaum speichern muss. Das heißt, die Subdomain
			\texttt{de} besitzt Informationen bezüglich den Subdomains \texttt{denic} und
			\texttt{uni-potsdam}, aber keine Informationen bezüglich \texttt{com} oder \texttt{net}.
			Dadurch wird einerseits die Menge an Informationen je Knoten gering gehalten, und außerdem
			kann so die Verwaltung und Pflege der Information an verschiedene Betreiber delegiert werden.

			Die Root-Domain, welche aktuell von 13 Root-Servern \cite{rootserver} bereitgestellt wird,
			steht unter der Kontrolle der Internet Corporation for Assigned Names and Numbers (ICANN). Auch
			die Vergabe der Top-Level-Domains (TLDs) ist an sie gebunden. Die ICANN ist eine
			Non-Profit-Organisation mit Sitz in den USA. Sie ist zwar für die Vergabe der TLDs
			verantwortlich, verwaltet und betrieben werden diese jedoch von verschieden Organisationen und
			Unternehmen. Es existieren aktuell 313 TLDs \cite{tlds}, welche sich hauptsächlich in folgende
			Arten unterteilen lassen:

			\begin{itemize}
				\item länderspezifische \textit{country-code TLDs} (ccTLDs) und
				\item allgemeine \textit{generic TLDs} (gTLDs):
				\begin{itemize}
					\item von ICANN verwaltete \textit{unsponsored TLDs} (uTLDs),
					\item von privaten Organisationen vorgeschlagene \textit{sponsored TLDs} (sTLDs).
				\end{itemize}
			\end{itemize}

			In der Abbildung \ref{fig:dns-tree} sind zum Beispiel die Domains \texttt{uk} und \texttt{de}
			ccTLDs, die Domains \texttt{com}, \texttt{org} und \texttt{net} sind uTLDs, und \texttt{mobi}
			ist eine sTLD. Unter den TLDs folgen die sogenannten Second-Level-Domains (SLDs), welche meist
			schon zu den eigentlichen Netzteilnehmern gehören oder noch eine weitere Einteilungsebene
			einführen. So gibt es zum Beispiel in der englischen \texttt{uk}-Domain noch die
			\texttt{co}-Domain für Firmen oder die \texttt{ac}-Domain für akademische Einrichtungen. Die
			\texttt{uni-potsdam}-Domain in der \texttt{de}-Domain wird allerdings bereits direkt von der
			Universität Potsdam verwaltet. Die Universität Potsdam unterteilt ihre eigene SLD wiederum
			weiter in Subdomains, zum Beispiel für die einzelnen Institute (\texttt{cs} für das Institut
			für Informatik). Diese Domains können dann wiederum von den einzelnen Instituten verwaltet und
			weiter unterteilt werden.

			\lstinputlisting[float,caption={Ausschnitt eines Master-Files},label=lst:master]{listings/master-file}

			Dieser dezentrale Ansatz hat eine Reihe von Vorteilen. Als wichtigster Punkt ist die
			Verteilung der Last zu nennen, womit sowohl der Aufwand zur Wartung einer Domain, als auch zur
			Verteilung der Informationen gemeint ist. Außerdem ist es für die Betreiber einer Domain
			einfacher Änderungen einzupflegen. Da ein Betreiber alle Subdomains seiner eigenen Domain
			allein verwalten kann, spricht man dabei von einer \textit{Zone}.
			Eine Zone umfasst alle Domains, die von diesem DNS-Betreiber administriert werden. So gehören
			zum Beispiel alle Subdomains von \texttt{haiti.cs.uni-potsdam.de} zu einer Zone. Diese Zonen
			werden über sogenannte Master-Files \cite{rfc1035,liualb2006} konfiguriert. Listing
			\ref{lst:master} zeigt einen Auszug aus einem Beispiel für ein Master-File. Der grundlegende
			Aufbau ist in der Form: Domain-Name (vollständig mit abschließendem Punkt oder Subdomain),
			Klasse (IN für Internet), Typ (optionale weitere Parameter für spezielle Typen z.B. bei MX)
			und abschließend die gespeicherte Information. In Listing \ref{lst:master} sind in Zeile 1-2
			zwei \texttt{NS}-Einträge. Diese geben die Nameserver einer Domain an. In Zeile 4 ist ein Eintrag
			für einen Mailserver (\texttt{MX}). Er enthält zusätzlich einen Präferenz-Wert (10),
			welcher bei mehreren Mailservern die Ordnung dieser definiert. Anschließend folgen in Zeile
			6-9 vier IPv4-Adressen (\texttt{A}) für verschiedene Subdomains. Und in Zeile 11 ist der Alias
			(\texttt{CNAME}) \textit{www} für die  \textit{twix}-Domain definiert. Dieses Beispiel zeigt
			nur eine kleine Anzahl von möglichen Typen, weshalb in Tabelle \ref{tab:dns-types} die
			wichtigsten von ihnen aufgeführt sind.

			\begin{table}
				\centering
				\begin{tabular}{|c|l|}\hline
					Typ & Beschreibung \\\hline\hline
					A	& IPv4-Adresse\\
					AAAA	& IPv6-Adresse\\
					NS	& Nameserver\\
					MX	& Mailserver\\
					CNAME	& Kanonischer Name eines Alias\\
					SOA	& Start einen Zonenautorität\\
					PTR		& Zeiger von eine Adresse auf eine Domain\\\hline
				\end{tabular}
			\caption{Die wichtigsten Eintragstypen in einem DNS-Master-File \cite{rfc1035,rfc3596}}
			\label{tab:dns-types}
			\end{table}

			Diese Master-Files werden von DNS-Servern gelesen und verarbeitet. Es gibt verschiedene Arten
			von DNS-Servern, die bestimmen, wie der DNS-Server auf Anfragen antwortet. Dabei
			unterscheidet man iterative und rekursive Nameserver.

			\subsubsection*{Iterative Nameserver} % {{{

			Ein iterativer Nameserver kennt nur die Informationen aus seinem eigenen Master-File, also
			seiner eigenen Zone. Erhält ein iterativer Nameserver eine Anfrage außerhalb seiner Zone, kann
			er diese nicht beantworten und sendet deshalb die Adressen der Server zurück, welche für die
			Root-Domain verantwortlich sind. Diese Adressen sind jedem DNS-Server bekannt. Das Ziel dieses
			Verhaltens ist es, auf dem DNS-Server keine unnötige Last zu erzeugen und die Antwortzeit so
			gering wie möglich zu halten. Daraus lässt sich auch der Haupteinsatzort für solche Nameserver
			ableiten. Alle Server, welche die Root-Domain und die TLDs verwalten, sind iterative
			Nameserver. Sie erhalten die meisten Anfragen und müssen diese schnell und effizient
			verarbeiten. Auch die meisten DNS-Server von Firmen, welche durch das Internet erreichbar
			sind, werden ebenfalls iterative Nameserver sein.

			% subsubsection Iterative Nameserver }}}

			\subsubsection*{Rekursive Nameserver} % {{{

			Das Gegenteil zu iterativen sind rekursive Nameserver. Auch sie beantworten Anfragen bezüglich
			der eigenen Zone. Wenn sie jedoch eine Anfrage außerhalb ihrer Zone erhalten, erfragen sie die
			Antwort selbst bei anderen Nameservern. Das heißt, der gefragte rekursive Nameserver
			unternimmt den Aufwand und ermittelt selbst die Antwort auf die gestellte Frage. Er sendet
			diese dann an den Fragenden zurück. Um dieses Vorgehen effizienter zu gestalten, werden häufig
			Caches eingesetzt. In ihnen werden die ermittelten Informationen, welche außerhalb der eigenen
			Zone liegen, gespeichert. Dadurch können Anfragen schneller beantwortet werden, falls die
			Antwort bereits im Cache existiert. Falls dies nicht der Falls ist, kann trotzdem schneller
			die Antwort gefunden werden, falls im Cache bereits Informationen zu einem Nameserver in der
			Nähe der Domain existieren. Das heißt, wenn die Domain \texttt{haiti.cs.uni-potsdam.de}
			angefragt wird und im Cache bereits die Adresse des \texttt{uni-potsdam.de}-Nameservers
			gespeichert ist, kann die Antwort schneller gefunden werden, als wenn der Nameserver sich von
			der Root-Domain durchfragen müsste.

			Rekursive Nameserver werden vor allem in privaten Netzwerken eingesetzt, wie zum Beispiel
			Heimnetzwerke oder firmeninternen Netzwerke, aber auch bei Internet Service Providern
			(ISPs). An diesen Punkten im Internet ist der Einsatz rekursiver Nameserver am sinnvollsten,
			denn dort treffen häufig die gleichen oder zu mindestens ähnliche Anfragen ein. So kann man
			oft davon ausgehen, dass diese Nameserver immer die aktuelle Adresse von \texttt{google.com} und
			\texttt{wikipedia.org} etc. im Cache gespeichert haben. Dieses Speichern von Informationen
			ist nicht nur vorteilhaft für die Nutzer, sondern verringert auch die Last auf anderen
			DNS-Servern im Internet. Ohne Nameserver mit Cache wäre die Last auf dem DNS-Netzwerk wohl um
			einiges höher. Allerdings sind die Informationen eines DNS-Servers nur begrenzt gültig.
			Jede Antwort enthält eine maximale Lebensdauer der Information, danach ist sie nicht mehr
			gültig und sollte nicht weiterverwendet werden. Das heißt auch, dass sie aus dem Cache entfernt
			und neu abgefragt werden muss. Dies ist jedoch nötig, da sonst nie Änderungen innerhalb einer Zone
			weiter propagiert würden.

			% subsubsection Rekursive Nameserver }}}

			\subsubsection*{Resolver} % {{{

				\begin{figure}
					\centering
					\includegraphics[width=\textwidth]{images/request}
					\caption{DNS-Anfragen für \texttt{www.cs.uni-potsdam.de}} % TODO: schön machen
					\label{fig:bsp-request}
				\end{figure}

				Damit eine Anwendung, wie zum Beispiel ein Webbrowser oder ein E-Mail-Client, Adressen
				auflösen kann, wird ein sogenannter Resolver benötigt. Seine Aufgabe ist es, für die
				Anwendung einen Domain-Namen zu der entsprechenden IP-Adresse aufzulösen. Das heißt, wenn
				der Resolver von einer Anwendung die Anfrage nach der Adresse \texttt{www.cs.uni-potsdam.de}
				erhält, erzeugt der Resolver eine gültige DNS-Anfrage \cite{rfc1035} nach eben dieser
				Adresse und sendet sie an den im Betriebssystem konfigurierten primären DNS-Server. Dabei
				handelt es sich meist um einen rekursiven Nameserver, welcher dann selber einen Resolver
				nutzt, um weitere Anfragen zu stellen, sofern er die Antwort nicht im Cache gespeichert hat.
				In Abbildung \ref{fig:bsp-request} ist der Ablauf der Beispiel-Anfrage dargestellt. Der
				Resolver des Clients sendet die Anfrage (1) an den lokalen DNS-Server. Dieser ist rekursiv
				und ermittelt deshalb nun die Antwort\footnote{Annahme: Der Cache des DNS-Servers enthält
				keine relevanten Informationen.}. Die Anfrage (2) wird deshalb an einen Root-Nameserver
				gestellt, da der lokale DNS keine anderen Informationen bezüglich der Domain besitzt. Der
				Root-Nameserver besitzt nur die Information, welcher Nameserver für die \texttt{de}-Domain
				verantwortlich ist und sendet diese Antwort (3) an den lokalen DNS-Server zurück. Dieser
				befragt (4) anschließend den \texttt{de}-Nameserver, welcher wiederum ein iterativer
				Nameserver ist und somit nur die \texttt{uni-potsdam}-Domain kennt (5). Die Anfragen an den
				\texttt{uni-potsdam}-Nameserver (6) liefert die Adresse des zuständigen Nameservers zurück
				(7). Dieser kann nun auf die letzte Anfrage (8) mit der gewünschten Adresse antworten (9)
				und der lokale DNS-Server liefert sie an den Client zurück (10). Alle Informationen, die in
				den Zwischenanfragen gesammelt wurden, werden im Cache des lokalen DNS-Servers gespeichert.
				Würde nun der Client erneut nach der Adresse \texttt{www.cs.uni-potsdam.de} fragen, könnte
				der lokale DNS-Server sofort antworten. Oder wenn die Anfrage diesmal
				\texttt{haiti.cs.uni-potsdam.de} wäre, würde der lokale DNS-Server sofort beim
				\texttt{cs.uni-potsdam.de}-Nameserver anfragen, da er dessen Adresse bereits kennt und weiß,
				dass dieser für die Domain zuständig ist.

			% subsubsection Resolver }}}

			\subsubsection*{DNS Security Extension} % {{{

				Mit der DNS Security Extension (DNSSEC) \cite{rfc4033,rfc4034,rfc4035,liualb2006} wurde ein
				neuer Standard eingeführt, welcher das DNS absichern soll. Es gibt verschiedene
				Angriffsszenarien auf das DNS \cite{lorenz2012}, wodurch Informationen verändert und somit
				ganze Domains übernommen werden können. Mit DNSSEC soll die Authentizität und
				Datenintegrität der Informationen gewährleistet werden und überprüfbar sein. Dazu werden
				kryptographische Verfahren eingesetzt. Die Einführung und Umstellung auf DNSSEC ist zu
				diesem Zeitpunkt noch nicht abgeschlossen, jedoch ergeben sich daraus interessante
				Überlegungen bezüglich der Last auf dem DNS-Netzwerk. Durch die kryptographische
				Absicherung werden die DNS-Nachrichten größer und Resolver müssen kryptographische
				Methoden nutzen, was eine höhere Rechenleistung beim Client voraussetzt. Sobald das
				System vollständig eingeführt wurde, wird es interessant sein zu untersuchen, in welcher
				Form sich die Veränderung auf die Auslastung des DNS auswirkt.

			% subsubsection DNSsec }}}

			% subsection Aufbau }}}

			\subsection{DNS-Server Software} % {{{
			\label{sub:software}
			
				In diesem Abschnitt werden mehrere Softwaresysteme vorgestellt, welche zum Betrieb eines
				DNS-Server genutzt werden können. Diese wurden aufgrund von zwei Studien aus den Jahren 2004
				und 2009 zur Verteilung von DNS-Server-Software ausgewählt. Die erste Studie
				\cite{survey2004} wurde am 23. Mai 2004 abgeschlossen und untersuchte 37.836.997 SLDs unter
				den TLDs \texttt{com}, \texttt{net}, \texttt{org},	\texttt{info} und \texttt{biz}. Die
				zweite Studie \cite{survey2009} ist von Oktober 2009. In ihr wurden 3.308.662 SLDs unter den
				TLDs \texttt{com}, \texttt{net} und \texttt{org} untersucht. Man versuchte jeweils die
				Version der eingesetzten DNS-Server-Software zu ermitteln. Dabei wurden die Systeme BIND,
				djbdns, Microsoft DNS und NSD als vier der häufigsten Systeme ermittelt (siehe Tabelle
				\ref{tab:verteilung}). Die genannten vier Systeme werden im Folgenden vorgestellt.
				Zusätzlich wird auf den dnsmasq Server eingegangen, welcher eine sehr spezielle Form eines
				DNS-Servers darstellt.
				
				\begin{table}
					\centering
					\begin{tabular}{|c|r|r|}\hline
						DNS-Server & \multicolumn{1}{c|}{2004} & \multicolumn{1}{c|}{2009} \\\hline\hline
						BIND & \unit[70,11]{\%} & \unit[73,85]{\%} \\
						djbdns & \unit[15,57]{\%} & \unit[2,56]{\%} \\
						Microsoft DNS & \unit[6,24]{\%} & \unit[0,26]{\%}\\
						NSD & \unit[0,20]{\%} & \unit[0.03]{\%} \\\hline
					\end{tabular}
					\caption{Verteilung von DNS-Server Software nach \cite{survey2004, survey2009}}
					\label{tab:verteilung}
				\end{table}

				\subsubsection*{BIND} % {{{

				Die erste Version des Berkeley Internet Name Domain (BIND) \cite{bind} Server wurde
				Anfang der achtziger Jahre an der University of California, Berkeley entwickelt und mit
				4.3BSD erstmals veröffentlicht. Inzwischen wird BIND von dem Internet Systems Consortium
				(ISC) weiterentwickelt und gepflegt. BIND hat sich zu dem De-facto-Standard für DNS-Server
				entwickelt. Er wird auf 10 der 13 Root-Server eingesetzt und ist auch sonst bei TLDs
				weitverbreitet. Die erste Version des aktuellen BIND 9 Servers wurde 2000 veröffentlicht.
				Ein Nachfolger wird seit 2 Jahren entwickelt, um die aktuellsten Standards (z.B. DNSSEC) zu
				unterstützen.

				Für diese Arbeit wurde der BIND Server als DNS-Server gewählt, da er der weitverbreitetste
				ist und ebenfalls am Institut für Informatik der Universität Potsdam eingesetzt wird.
				Dadurch konnten Testmessungen in einem realistischen Umfeld mit realen Server-Logs
				ausgeführt werden.

				% subsubsection BIND }}}

				\subsubsection*{Name Server Daemon (NSD)} % {{{

				Der Name Server Daemon (NSD) \cite{nsd} wird von den NLnet Labs entwickelt und sein
				Hauptziel ist die Vielfalt unter den Root-Servern zu erhöhen.  Das soll dazu führen, dass
				nicht alle Root-Server durch einen Fehler im BIND Server angreifbar sind. Aktuell nutzen
				auch bereits 3 der 13 Root-Server den NSD als DNS-Server.  Mit der Zielsetzung, vor allem
				auf Root-Servern und TLD-Servern eingesetzt zu werden, erklären sich auch die meisten
				Charakteristiken des NSD. Er ist ausschließlich ein autoritativer DNS-Server, das heißt, er
				beantwortet keine rekursiven Anfragen. Dadurch ist sein Einsatz als DNS-Server in einem
				lokalen Netzwerk ausgeschlossen. Bei dem NSD kann deshalb auf unnötige Funktionen verzichtet
				und somit auf hohe Belastbarkeit optimiert werden. Die erste Version des aktuellen NSD 3
				wurde 2006 veröffentlicht.

				% subsubsection Name Server Daemon (NSD) }}}

				\subsubsection*{Microsoft DNS} % {{{

				Der Microsoft DNS Server \cite{msdns} ist eine Implementation von Microsoft, welche mit den
				Windows Server Produkten ausgeliefert wird. Er kann als autoritativer und rekursiver
				Nameserver betrieben werden. Durch seinen Einsatz auf Windows Servern kommt er vor allem in
				lokalen und Firmen-Netzwerken vor. Ein besonderes Merkmal des Microsoft DNS Servers ist,
				dass die DNS-Informationen nicht nur aus einem Master-File gelesen werden, sondern auch aus
				einem Active Directory stammen können.

				% subsubsection Microsoft DNS }}}

				\subsubsection*{djbdns} % {{{

				Das von Daniel J. Bernstein entwickelte Softwarepaket djbdns \cite{djbdns} ist eines der am
				häufigsten benutzten Systeme zum Betrieb eines DNS-Servers. Daniel J.  Bernstein ist ein
				Mathematiker, Programmierer und Kryptologe aus den USA. Er veröffentlichte djbdns im Jahr
				2001 und setzte eine Belohung \cite{guarantee} von \$1000 aus, für den ersten, der einen
				Fehler in djbdns findet. Die Besonderheit von djbdns liegt darin, dass es ein Softwarepaket
				aus mehreren Programmen ist, welche immer einen speziellen Einsatzzweck erfüllen. Somit
				verringert sich die Code-Basis und die Komplexität der Programme. Die Hauptkomponenten sind
				\texttt{tinydns}, ein autoritativer, und \texttt{dnscache}, ein rekursiver
				Nameserver. Für andere Funktionen, wie das Austauschen von kompletten Zone-Informationen oder
				Blacklisting, existieren ebenfalls eigene Programme.

				% subsubsection Djbdns }}}

				\subsubsection*{dnsmasq} % {{{

				Das Programm dnsmasq \cite{dnsmasq} wird ausschließlich für Heimnetzwerke entwickelt. Es
				stellt DNS- und DHCP\footnote{Das Dynamic Host Configuration Protocol (DHCP) \cite{rfc2131}
				ermöglicht es, Clients in einem Netzwerk dynamisch IP-Adressen zuzuweisen. So kann zum
				Beispiel in einem Heimnetzwerk auf die Konfiguration von statischen IP-Adressen verzichtet
				werden.}-Dienste für ein kleines Netzwerk zur Verfügung. Die Grundidee besteht darin, dass dnsmasq
				die Netzwerkknoten im Heimnetz über DNS verfügbar macht, indem es entweder die
				\texttt{/etc/hosts} oder DHCP Informationen nutzt. Dadurch müssen die lokalen Rechner keine
				eigene \texttt{hosts}-Datei pflegen.  Zusätzlich bietet es einen einfachen Cache für
				DNS-Anfragen, was zu einer Beschleunigung im lokalen Netz führen kann. Somit ist dnsmasq
				kein üblicher DNS-Server und eignet sich auch nicht zum Verwalten von Domains, zeigt jedoch
				eine alternative resourcenschonende lokale Verwendung des DNS auf.

				% subsubsection Dnsmasq }}}

			% subsection DNS-Server Software }}}

		% section Domain Name System (DNS) }}}

		\section{Lastverteilung} % {{{
		\label{sec:lastverteilung}

		\begin{figure}
			\centering
			\includegraphics[width=5cm]{images/internet-server}
			\caption{Lastverteilung mit mehreren Servers}
			\label{fig:lastverteilung}
		\end{figure}

		In diesem Abschnitt sollen Problemlösungen vorgestellt werden, welche dazu dienen die Last einer
		einzelnen Netzwerkressource zu verringern. Dies ist immer dann nötig und sinnvoll, wenn eine
		einzelne Ressource im Netzwerk einer so hohen Last ausgesetzt ist, dass sie diese nicht mehr
		allein abarbeiten kann. Bei zu hoher Last können einzelne Anfragen an die Ressource nicht mehr
		beantwortet werden oder im ungünstigsten Fall droht ein Totalausfall. Ein Beispiel für hohe
		Lastsituationen sind Webserver, welche ab einer gewissen Anzahl von Anfragen pro Sekunde nicht
		mehr in der Lage sind, alle abzuarbeiten. Um die Überlastung einzelner Ressourcen zu verhindern
		gibt es zwei Möglichkeiten. Die erste Variante ist, die entsprechende Ressource durch eine
		leistungsfähigere Variante auszutauschen. Dies wird oft bei Datenbanken genutzt, wo der Server
		durch neue Hardware und bessere Ausstattung erweitert wird.  Für Datenbanken ist dies oft die
		einfachere Lösung, da es problematisch sein kann, sie zu verteilen und dabei die Daten
		konsistent zu halten. Die andere Variante ist eine Skalierung in die Breite, das heißt, zum
		Beispiel weitere Server bereitzustellen. So kann der Dienst des Servers, wie in Abbildung
		\ref{fig:lastverteilung} zu sehen ist, von mehreren Servern dem Internet bereitgestellt werden.
		Dieses Vorgehen eignet sich oft bei Webservern, welche durch die Auslieferung von dynamischen
		Inhalten (z.B. PHP-Seiten) ausgelastet sind. Das Skalieren in die Breite bietet Vorteile
		gegenüber der ersten Variante und ist deshalb dieser vorzuziehen, sofern es die Anwendung
		zulässt. Allein durch die Bereitstellung eines zusätzlichen Servers ist es einfacher, einzelne
		Server zu warten und die Gefahr eines Totalausfalls verringert sich. Durch diese Verteilung kann
		ein einzelner Server ausfallen oder überprüft werden, währende der andere Server die Anwendung
		weiterhin zur Verfügung stellt. Zudem ist die Erweiterbarkeit zur weiteren Skalierung einfacher,
		wenn bereits die Infrastruktur geschaffen wurde, mehrere Server zu nutzen. Dieses Variante ist
		im Vergleich zur vorherigen kostengünstiger, da die Anschaffung zweier identischer Systeme meist
		preiswerter als ein einzelnes System ist, welches die gleiche Last verarbeiten kann.  Hinzu
		kommt die Möglichkeit, gezielt Ressourcen zu entfernen, um Strom zu sparen. So könnten Server
		nur dann angeschaltet werden, wenn davon auszugehen ist, dass eine hohe Last bevorsteht. Ein
		Beispiel hierfür wäre eine Verkaufsplattform, die zusätzliche Ressourcen benötigt, wenn zum
		Beispiel vor Feiertagen eine hohe Anzahl Bestellungen eingeht.

		Im Folgenden werden verschiedene Ansätze zur Verteilung von Last auf mehrere Systeme erläutert.
		Sie unterscheiden sich in der Umsetzung der Lastverteilung und bieten jeweils gewisse Vor- und
		Nachteile. Die Hauptanwendung für diese Systeme ist die Verteilung von HTTP-Anfragen.
		Diese unterscheiden sich von DNS-Anfragen in mehreren Punkten, weshalb anschließend noch
		Besonderheiten und existierende Verfahren der Lastverteilung von DNS-Verkehr erklärt werden.
	
			\subsection{Server-Lastverteilung} % {{{
			\label{sub:Server-Lastverteilung}
			
			\begin{figure}
				\centering
				\includegraphics[width=5cm]{images/loadbalancer}
				\caption{Lastverteilung mit Hilfe eines Lastverteilers}
				\label{fig:loadbalancer}
			\end{figure}

			Bei dem Konzept der Server-Lastverteilung wird von der folgenden Situation ausgegangen.  Es
			existiert ein Cluster aus mehreren Servern, welche alle die selbe Anwendung (z.B. eine
			Website) zur Verfügung stellen. Diese sogenannten Backend-Server sind über einen
			Lastverteiler, einem weiteren Server, mit dem restlichen Netzwerk oder Internet verbunden
			(siehe Abbildung \ref{fig:loadbalancer}). Der Lastverteiler hat die folgenden Aufgaben
			\cite{bourke2001}:

			\begin{itemize}
				\item Entgegennehmen des kompletten Netzwerkverkehrs, welcher an die Anwendung gerichtet ist
					(z.B. HTTP-Anfragen an eine Website).
				\item Verteilung des angenommenen Netzwerkverkehrs auf die Backend-Server.
				\item Überwachung der Last der einzelnen Backend-Server und entsprechende Anpassung der
					Verteilung des Netzwerkverkehrs.
			\end{itemize}

			Diese Anforderungen können durch unterschiedliche Systeme realisiert werden. Ein Lastverteiler
			kann im Kernelspace oder Userspace des Betreibssystems
			implementiert sein. Ansätze für Userspace-basierte Lastverteiler sind zum Beispiel das Programm
			perlbal \cite{perlbal} oder das Apache-Modul mod\_proxy\_balancer \cite{modproxy}.  Eine
			Implementierung im Userspace hat den Vorteil, dass der Lastverteiler unabhängig von konkreten
			Betriebssystemen und Kernel-Versionen ist. Außerdem benötigt dieser keine \texttt{root}-Rechte
			um konfiguriert und gestartet zu werden. Da es sich um Anwendungen im Userspace handelt,
			findet hier die Lastverteilung auf einer höheren OSI-Schicht \cite{tanenbaum1988} statt.
			Dadurch stehen dem Lastverteiler anwendungsspezifischere Informationen zu einer Anfrage
			bereit. Dies kann dazu genutzt werden, die Abfragen nach deren Inhalt zu filtern
			und so ein kontextsensitives Verteilen zu ermöglichen.  Ein Nachteil, der vor allem bei
			hohem Netzwerkverkehr entsteht, ist der nötige Kontextwechsel, damit der Lastverteiler
			im Userspace die Netzwerkpakete auswerten kann. Dazu muss das Netzwerkpaket in den Userspace
			und zum Weiterversenden wiederum in den Kernelspace kopiert werden. Hierbei ist jeweils ein
			Kontextwechsel nötig, welcher bei hohem Netzwerkverkehr zu Leistungseinbußen führen kann
			\cite{boehme2006}.  Im Vergleich zu diesen Ansätzen im Userspace gibt es im Kernelspace den
			Linux Virtual Server (LVS) \cite{lvs,zhang2000} für Linux und den Paketfilter pf \cite{pf} für
			OpenBSD als bekannteste Beispiele.  Indem die Pakete bereits im Kernelspace verarbeitet
			werden, findet kein Kontextwechsel statt und die Verarbeitung kann schneller erfolgen.
			Allerdings findet die Verarbeitung auf einer der unteren Schichten des OSI-Modells statt, was
			dazu führt, dass keine anwendungsspezifischen Informationen vorliegen. Jedoch bietet dieser
			Ansatz eben genau dies auch als Vorteil, denn so kann eine Lastverteilung unabhängig von
			Anwendungen stattfinden.  Das heißt, ein Lastverteiler, wie der LVS, kann so konfiguriert
			werden, dass er gleichzeitig unterschiedlichen Netzwerkverkehr (z.B. unterschiedliche
			Transportprotokolle oder Portnummern) separat verteilt. Dies ist mit einer anwendungsbasierten
			Lösung nicht ohne weiteres realisierbar.

			Unabhängig von der gewählten Software-Lösung ist der gewählte Algorithmus zur Lastverteilung
			ein entscheidendes Kriterium. Es existieren mehrere Algorithmen für dieses Problem
			\cite{zinke2007}, wobei es von der gewählten Software abhängt, welche überhaupt zur Verfügung
			stehen.  Die zwei bekanntesten Verfahren sind das Round-Robin-Verfahren und das
			Least-Connection-Verfahren. Beim Round-Robin-Algorithmus wählt der Lastverteiler aus einer
			Liste von Servern den nächsten aus. Das geschieht zyklisch, was dazu führt, dass die
			Reihenfolge der Server immer gleich bleibt. Dies ist ein sehr einfacher Algorithmus der aber
			bei homogenen Clustern und homogenen Anfragen ausreicht, um die Last sinnvoll zu verteilen.
			Handelt es sich nicht um ein homogenes Cluster, so kann durch eine gewichtete Variante des
			Algorithmus die Last entsprechend der Leistung der Server verteilt werden. Sind jedoch die
			Anfragen an das Cluster nicht homogen, das heißt, jede Anfrage erzeugt eine unterschiedliche
			Last bei den Backend-Servern, kann das nicht durch den Round-Robin-Algorithmus ausgeglichen
			werden. Der zweite bekannte Algorithmus ist Least-Connection. Dieser Algorithmus beachtet bei
			der Verteilung die aktuelle Anzahl an Verbindungen zu den Backend-Servern. Arbeitet ein Server
			schneller seine Anfragen ab als ein anderer, erhält dieser Server früher wieder
			Anfragen. Auch von diesem Algorithmus gibt es eine gewichtete Variante, welche die bereits
			dynamisch getroffene Entscheidung noch einmal verbessern soll. Daher ist anzunehmen, dass
			Least-Connection bessere Ergebnisse auf heterogenen Clustern als Round-Robin erzielt und auch
			nicht homogene Anfragen begrenzt abfängt.

			% subsection Server-Lastverteilung }}}

			\subsection{DNS-basierte Lastverteilung} % {{{
			\label{sub:dns-lastverteilung}

			\begin{figure}
				\centering
				\includegraphics[width=9cm]{images/dns-loadbalancer}
				\caption{Lastverteilung mit Hilfe von DNS}
				\label{fig:lastverteilung-dns}
			\end{figure}

			Existiert kein Cluster oder soll auf einen zusätzlichen Server verzichtet werden, bietet sich
			eine DNS-basierte Lastverteilung an. Diese nutzt die Möglichkeit, dass einem DNS-Eintrag für
			eine Domain mehrere IP-Adressen zugeordnet werden können \cite{rfc1034}. Ein möglicher
			Verbindungsablauf ist in Abbildung \ref{fig:lastverteilung-dns} dargestellt. Hier wird als
			erstes der DNS-Server befragt, welcher dann eine Liste aller möglichen Adressen liefert. Dabei
			existiert keine Ordnung dieser Einträge und der DNS-Server entscheidet, in welcher Reihenfolge
			er die IP-Adressen zurückliefert. Oft wird dazu ein Round-Robin-Algorithmus genutzt und somit
			eine gewisse Lastverteilung erreicht. Dieses Verfahren hat jedoch einige Nachteile. So führt
			das Caching von lokalen Nameservern dazu, dass alle Anfragen zu derselben Adresse geleitet
			werden. Weiterhin hat der DNS-Server keine Informationen über die Last der Server oder
			darüber, ob diese überhaupt erreichbar sind. Dadurch wird ein Server, der ausgefallen ist,
			trotzdem immer wieder als Antwort zurückgeliefert. Es ist auch nicht möglich, Gewichte für die
			Verteilung der Last auf den Server anzugeben, weshalb eine Heterogenität der Server nicht
			ausgeglichen werden kann. Dieses Problem sollte durch den DNS-Typen \texttt{SRV}
			\cite{rfc2782} behoben werden.  Ein Beispiel eines \texttt{SRV}-Eintrags für einen Webserver
			ist in Tabelle \ref{tab:srv} zu sehen. Durch den Domain-Namen wird definiert, dass dieser
			Eintrag nur für HTTP-Anfragen über TCP an die Domain \texttt{bsp.de} gilt. In dem Beispiel
			wird anschließend den Domains \texttt{www.bsp.de} und \texttt{www2.bsp.de} eine Priorität von
			0 und \texttt{backup.bsp.de} von 1 zugeordnet. Dabei muss von einem Client der Server mit der
			kleinsten Priorität gewählt werden, sofern dieser erreichbar ist. Gibt es mehrere Server mit
			derselben Priorität (wie im Beispiel \texttt{www.bsp.de} und \texttt{www2.bsp.de}), sollte die
			Wahl eines Server mit einem höheren Gewicht wahrscheinlicher sein. Zusätzlich wird noch der
			Port für das Ziel angegeben.  Dieses Verfahren führt zwei Gewichtungen ein, welche auf der
			Client-Seite ausgewertet werden.  Allerdings ist die Unterstützung auf der Clientseite für
			\texttt{SRV}-Einträge sehr gering. Zusätzlich ergibt sich noch, dass der DNS-Server nicht die
			Last und Funktionstüchtigkeit der Server kennt.  Jedoch gibt es dem Client einen Anhaltspunkt,
			welchen Server er als nächstes kontaktieren sollte, sofern ein Server nicht erreichbar ist.

			\begin{table}
				\centering
				\begin{tabular}{|ccccccl|}\hline
				 Domain & Klasse & Typ & Priorität & Gewicht & Port & Ziel \\\hline\hline
					\_http.\_tcp.bsp.de. & IN & SRV & 0 & 2 & 80 & www.bsp.de. \\	
					& IN & SRV & 0 & 1 & 80 & www2.bsp.de. \\	
					& IN & SRV & 1 & 0 & 80 & backup.bsp.de. \\\hline
				\end{tabular}
				\caption{Beispiel für einen \texttt{SRV}-Eintrag für einen Webserver}
				\label{tab:srv}
			\end{table} 
			
			Mit Geotargeting oder auch GeoIP genannt, ist es möglich, anhand der IP-Adresse eine Aussage
			über die ungefähre geografische Position des Inhabers zu treffen. Diese Information wird zum
			Beispiel beim Global Server Load Balancing (GSLB) \cite{bourke2001} verwendet, wodurch der
			DNS-Server seine Antwort an die geografische Position des Fragestellers anpassen kann. Der
			Gedanke dabei ist, dass ein geografisch naher Server auch der schnellere Server ist. Dies muss
			nicht immer der Fall sein und außerdem kann ist es möglich, dass der Client über einen DNS-Server
			Anfragen stellt, welcher selbst nicht geografisch nah ist. Dadurch kann es zu negativen
			Effekten kommen.

			Auch Kombinationen der Server-Lastverteilung und DNS-Lastverteilung sind möglich. Dabei wird ein
			DNS-Server um Lastverteilungsfunktionen erweitert. So beschreibt \cite{chyuyi2003} ein
			Verfahren, bei dem der DNS-Lastverteiler die Zeit überwacht, welche jeder Backend-Server für
			die Abfrage einer Antwort benötigt. Dadurch kann die DNS-Antwort entsprechend der
			Lastsitutation angepasst werden. Einen ähnlichen Ansatz findet man in \cite{mookim2005}. Dabei
			werden Backend-Server, die überlastet sind, aus der Round-Robin-Liste des DNS-Servers entfernt
			und dynamisch wieder hinzugefügt, sobald sie die Überlastsitutation überwunden haben.

			% subsection DNS-basierte Lastverteilung }}}
			
			\subsection{Lastverteilung von DNS-Verkehr} % {{{
			\label{sub:lastverteilung-dns}

			Die Verteilung von DNS-Anfragen unterscheidet sich in mehreren Punkten von HTTP-Anfragen. So
			ist das genutzte Transport-Protokoll UDP, statt TCP, und die Nachrichten dürfen nicht größer als
			512 Byte sein \cite{rfc1035}. Weiterhin existieren beim DNS keine Client-Sessions wie bei
			zustandsbehafteten HTTP-Anwendungen, wo ein Client während einer Session immer mit demselben Server
			kommunizieren muss. Die Lastverteilung von DNS-Anfragen tritt jedoch wesentlich seltener auf
			als bei HTTP-Anfragen. Firmen und Organisationen haben selten eine so hohe Anzahl an
			DNS-Anfragen, dass sie Überlastsituationen erreichen. Dies schließt nicht aus, dass mehrere
			DNS-Server existieren, welche aber oft nur als Schutz vor Ausfällen und anderen Störungen
			dienen.  In lokalen Netzwerken und kleinere Firmen reicht es daher aus, Lastverteilung mit Hilfe
			von DNS-Einträgen durchzuführen. Bei größeren Firmen, die eine höhere Anzahl an DNS-Anfragen
			zu bewältigen haben, ist der Einsatz eines DNS-Clusters und Server-Lastverteilers sinnvoll. Die
			größte Last liegt bei den Betreibern von TLDs und den Root-Servern. Für diese Anwendungsfälle
			sind DNS-Cluster nötig. Zusätzlich kommt Anycast \cite{rfc4786, sapate2005} zum Einsatz. Dabei
			werden mehrere DNS Server mit derselben Anycast-IP-Adresse mit dem Internet verbunden. Sendet
			nun ein Client eine Anfrage an diese Anycast-Adresse, antwortet der Server, der als erster die
			Frage erhält. Die Idee dabei ist, dass durch den DNS-Server, der die kürzeste Route zum
			Fragesteller hat, die beste Antwort gegeben werden kann. Vor allem bei den Root-Servern ist
			das sinnvoll. Es existieren insgesamt 13 Root-Server
			(A-M) \cite{rootserver}. Von ihnen nutzen 9 (A, C, F, G, I, J, K, L und M)
			Anycast-Adressen. Die anderen 4 Root-Server haben nur lokale Standorte in den USA. 
				
			% subsection Lastverteilung von DNS-Verkehr }}}

		% section Lastverteilung }}}

		\section{Fazit} % {{{
		\label{sec:grundlagen-fazit}
		
			% Einordnung der Arbeit

		% section Fazit }}}

  % chapter Grundlagen }}}

	\chapter{Verwandte Arbeiten} % {{{
	\label{cha:arbeiten}

	% TODO: DNS Measurements at a Root Server
	% TODO: DNS Cluster
	% TODO: DNS Root/gTLD Performance Measurement
	% TODO: Measurement of DNS Traffic Caused by DDoS Attacks
	% TODO: Measuring Availability in the Domain Name System
	% TODO: Two Days in the Life of the DNS Anycast Root Servers

	% chapter Verwandte Arbeiten }}}

	\chapter{salbnet} % {{{
	\label{cha:salbnet}

		Dieses Kapitel beschreibt salbnet \cite{salbnet}, einen Credit-basierten
		Lastverteilungsansatz.  Er basiert auf Arbeiten zur selbst-adaptiven Lastverteilung von
		TCP-Verkehr vor allem in InfiniBand-Netzwerken \cite{zinke2007, scsczile2008,
		schneidenbach2009}. Die Besonderheit an diesem Lastverteilungsverfahren ist, dass es sowohl eine
		Komponente auf dem Lastverteiler als auch auf den Backend-Servern gibt. Dabei werden auf den
		Backend-Servern Credits bestimmt, welche die Verfügbarkeit des Servers wiederspiegeln sollen,
		und an den Lastverteiler übermittelt. Der Lastverteiler wählt dann anhand der gemeldeten
		Credits die Backend-Server aus. Dieser Aufbau von salbnet ist in Abbildung \ref{fig:salbnet} zu
		sehen, wobei die Komponenten von salbnet blau hervorgehoben sind. Sie sollen im Folgenden kurz
		vorgestellt werden.

		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{images/salbnet.png}
			\caption{Aufbau von salbnet (übernommen aus \cite{zinke2012})}
			\label{fig:salbnet}
		\end{figure}

		\subsection*{libnetmsg} % {{{

			Die libnetmsg \cite{rabweg2009} ist eine leichtgewichtige Kommunikations-Bibliothek die zur
			Übertragung sowohl UDP und TCP als auch InfiniBand unterstützt. Außerdem kann bei InfiniBand
			die Remote Direct Memory Access (RDMA) Eigenschaft genutzt werden. In salbnet wird die
			Bibliothek eingesetzt um die Credits von den Backend-Server an den Lastverteiler zu melden. Wird
			InfiniBand verwendet können die neuen Credits mit Hilfe von RDMA direkt in den Kernelspeicher
			des Lastverteilers geschrieben werden. Dieses Vorgehen ist in Abbildung \ref{fig:salbnet}
			dargestellt.

		% subsection libnetmsg }}}
		
		\subsection*{libnethook} % {{{

			Mit der dynamische Bibliothek libnethook können Funktionsaufrufe, von Anwendungen überwacht
			werden und so kann ermittelt werden wie oft eine Funktion aufgerufen bzw. was von ihr zurückgegeben wird. Im Fall von salbnet, wird der \texttt{accept}-Funktionsaufruf vom Apache
			Webserver (httpd) \cite{httpd} abgefangen und analysiert. In Abbildung \ref{fig:salbnet} ist
			angedeutet, dass libnethook zwischen dem httpd und der libc agiert. Allgemein kann das auch
			für alle anderen TCP-Anwendungen instrumentalisiert werden. Im TCP-Fall wird nur die Anzahl der
			Funktionsaufrufe gezählt, um eine Neuberechnung der Credits auszulösen.

		% subsection libnethook }}}

		\subsection*{salbd} % {{{

			Die Hauptkomponente von salbnet ist salbd, welches sowohl auf dem Backend-Server als auch auf
			dem Lastverteiler zum Einsatz kommt. Auf dem Lastverteiler stellt salbd einmal ein Kernelmodul
			bereit, das den salbnet Scheduler für LVS implementiert. Dieser wählt Round-Robin den
			nächsten Backend-Server aus allen Servern aus, welche noch Credits größer 0 besitzen. Außerdem
			wird salbd genutzt, um die Credits von den Backend-Servern zu empfangen und bei Übertragungen
			ohne RDMA, auch um die Credits in den Kernelspace zum salbnet Scheduler zu übermitteln.  Im
			Backend ist salbd zur Bestimmung der Credits anhand der aktuellen Last der Webserver
			verantwortlich. Dazu werden verschiedene Metriken verwendet und anhand der ermittelten Werte
			verschiedene Meldestrategien verfolgt. Dieser Ablauf ist schematisch in Abbildung
			\ref{fig:salbd} dargestellt. Dabei überwacht salbd die ensprechende Anwendung (z.B.
			httpd), ermittelt dadurch Metriken, welche genutzt werden, um Credits zu berechnen. Diese
			werden dann zum Lastverteiler übermittelt.

		% subsection salbd }}}

		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{images/salbd-concept.png}
			\caption{Konzept von salbd (übernommen aus \cite{zinke2012})}
			\label{fig:salbd}
		\end{figure}

		\section{Metrik und Credits} % {{{
		\label{sub:metrik}

			In der Dissertation \cite{zinke2012} werden die Forschungsergebnisse über salbnet vorgestellt.
			Dabei ist die Idee von salbnet Credits zur Bewertung von Lastsitutationen zu nutzen. Das bedeutet,
			dass die Backend-Server ihre aktuelle Last ermitteln. Danach bestimmen die Backend-Server eine
			Credit-Anzahl, diese soll der Anzahl von Anfragen entsprechen, welche zum aktuellen Zeitpunkt
			zusätzlich vom Server abgearbeitet werden könnten. Diese Anzahl an Credits werden dem
			Lastverteiler gemeldet. Der Lastverteiler entscheidet mit einem einfachen Round-Robin-Verfahren,
			welcher Server die nächste Anfrage erhält.  Dazu werden alle Server, welche noch Credits
			besitzen,	durchiteriert. Wird eine Anfrage einem Server zugewiesen, werden dessen Credits um eins
			verringert. Fallen die Credits eines Servers auf 0 und sendet der Server keine neuen Credits,
			wird der Server aus der Liste der verfügbaren Server entfernt. Meldet ein entfernter Server,
			erneut Credits, wird er wieder in die Verteilung aufgenommen. Die
			beiden wichtigsten Komponenten dieses Algorithmus sind die Metrik zur Berechnung der Credits und
			die Meldestrategie der Backend-Server.

			Die Metrik hat folgende Anforderungen zu erfüllen \cite{scsczile2008}:

			\begin{itemize}
				\item Sie sollte einfach zu berechnen sein.
				\item Sie sollte die aktuell zur Verfügung stehenden Ressourcen des Servers angeben.
				\item Sie sollte applikationsunabhängig sein.
				\item Sie sollte auch für Dienste mit mehreren Tausend Clients skalieren.
			\end{itemize}

			Die Version von salbnet vor dieser Arbeit war auf TCP-Anfragen begrenzt. Dazu wurde eine
			Metrik beruhend auf der TCP-Backlog des Webservers entwickelt. Die Backlog erfüllt alle
			gestellten Anforderungen an eine gute Metrik. Sie ist einfach zu berechnen, da der aktuelle
			Füllstand und der maximale Füllstand leicht abfragbar sind. Die Backlog des Webservers
			entspricht einer Warteschlange für den Server. Solange die Backlog leer ist, kann der
			Webserver alle Anfragen sofort beantworten. Wenn die Anfragen in der Backlog gespeichert
			werden, ist der Server nicht mehr zu einer sofortigen Abarbeitung in der Lage. Die wartenden
			Anfragen werden nun in der Backlog gespeichert. Das heißt, die Anzahl der noch freien
			Plätze in der Backlog gibt die Anzahl der noch möglichen Verbindungen an, bevor die Backlog
			voll ist. Ist die Backlog voll und es treffen weitere Anfragen ein, werden diese verworfen.
			Somit kann die Anfrage nicht beantwortet werden.  Daher entspricht der aktuelle Füllstand der
			Backlog genau den aktuell zur Verfügung stehenden Ressourcen des Servers. Dadurch, dass die
			TCP-Backlog überwacht wird, ist diese Methode allgemein für alle Webserver nutzbar und somit
			applikationsunabhängig. Außerdem benötigt die Metrik keine besonderen Ressourcen, das heißt, sie
			ist weder Speicher- noch CPU-intensiv, und somit beeinflusst sie die Leistung des
			Webservers nicht.

			Die Betrachtung der Backlog zeigt auch zwei wichtige Metriken zur Bewertung einer Lastverteilung
			für Webserver. Das Verwerfen einer Anfrage muss mit allen Möglichkeiten verhindert werden. Jede
			verworfene Anfrage kann einem verlorenen Nutzer oder Kunden entsprechen. Muss eine Anfrage sehr
			lange in der Backlog warten, ist dies zwar nicht so ungünstig, wie eine verworfene Anfrage, aber
			es bedeutet eine längere Wartezeit für den Nutzer. Wenn dieser nicht bereit ist, so lange auf eine
			Antwort zu warten, hat man einen Nutzer oder Kunde verloren. Das heißt, eine gute
			Lastverteilung sollte zu einer geringen Zahl an verworfenen Anfragen und einer minimalen
			Antwortzeit führen.

		% section Metrik und Credits }}}

		\section{Meldestrategien} % {{{
		\label{sub:meldestrategien}

			Als zweiter wichtiger Faktor des Credit-Ansatzes zählt neben der Metrik die Meldestrategie. Dabei
			ist es entscheident, wieviele und wie oft Credits gemeldet werden. Eine häufige Meldung von Credits
			könnte eine zusätzliche Last auf den Backend-Servern erzeugen. Werden Credits zu selten
			gemeldet,
			kann es sein, dass der Backend-Server beim Lastverteiler aussortiert wird, weil seine Credits auf
			0 gefallen sind. Folgend werden 4 mögliche Meldestrategien vorgestellt \cite{scsczile2008, schneidenbach2009}.

			\paragraph{Plain} % {{{
			\label{par:plain}

				Plain stellt die einfachste Lösung vor. Dabei wird immer nach einem fixen Intervall die
				Differenz zwischen der maximalen Größe der Backlog und dem aktuellen Füllstand der Backlog
				gemeldet. Dies entspricht allen aktuell verfügbaren Ressourcen.

			% paragraph Plain }}}

			\paragraph{Soft + Hard Credits} % {{{
			\label{par:soft-hard-credits}

			Bei diesem Algorithmus werden dem Lastverteiler zweimal Credits mitgeteilt. Die Hard-Credits
			entsprechen den aktuell verfügbaren Ressourcen wie schon beim Plain-Algorithmus. Die Soft-Credits
			sind eine Empfehlung vom Backend-Server an den Lastverteiler. Sie ergeben sich aus den
			Hard-Credits und der aktuellen Lastsitutation, das heißt, wenn der Server Probleme hat die Backlog
			abzuarbeiten, sinken die Soft-Credits. Somit soll verhindert werden, dass zu viele Requests an den
			schon unter Last stehenden Server gesendet werden. Für den Lastverteiler sind grundsätzlich die
			Soft-Credits das ausschlaggebende Maß. Erst, wenn kein Server mehr über Soft-Credits verfügt,
			werden die Hard-Credits beachtet.

			% paragraph Soft + Hard Credits }}}

			\paragraph{Dynamic Report} % {{{
			\label{par:dynamic-report}

			Dynamic Report ersetzt das feste Meldeintervall des vorherigen Algorithmus durch ein dynamisch
			berechnetes. Dabei bildet ein festes Intervall eine Untergrenze. Dies verhindert eine Meldung nach
			jeder abgearbeitet Anfrage. Dieser Algorithmus meldet oft Credits, wenn die Backlog nur leicht
			gefüllt ist und eine geringe Last	auf dem Server liegt. Ist die Backlog stark gefüllt und der
			Server mit der Abarbeitung der Anfragen beschäftigt, werden die Credits seltener gemeldet. Das
			führt dazu, dass bei vielen Anfragen keine zusätzliche Last durch unnötige Credit-Meldungen entsteht
			und bei geringer Last der Lastverteiler immer aktuelle Werte vom Backend-Server besitzt.

			% paragraph Dynamic Report }}}

			\paragraph{Dynamic Pressure Relieve} % {{{
			\label{par:dynamic-pressure-relieve}

			Der Dynamic Pressure Relive Algorithmus kombiniert alle vorherigen Algorithmen. Es existieren
			wiederum Hard- und Soft-Credits. Allerdings werden die Hard-Credits spätestens nach einem
			fixen Intervall und die Soft-Credits nach dem dynamischen Intervall aus dem Dynamic Report
			Algorithmus gemeldet. Bei einer Meldung der Soft-Credits an den Lastverteiler werden
			gleichzeitig die Hard-Credits mitgesendet. Dieses Vorgehen soll verhindern, dass zu selten
			Hard-Credits bei hoher Last gemeldet werden, was zu einer unnötig hohen Anzahl an verworfenen
			Anfragen führen kann.

			% paragraph Dynamic Pressure Relieve }}}

			Bei einer Simulation der verschiedenen Meldestrategien hat der Dynamic Pressure Relieve
			Algorithmus die besten Ergebnisse erzielt \cite{scsczile2008}.

		% section Meldestrategien }}}

		\section{Benchmark} % {{{
		\label{sec:Benchmark}

			Um salbnet auf Webclustern zu testen, wurde der Benchmark servprep/servload \cite{habenschuss2011}
			entwickelt. Dabei ist servprep ein Programm, welches Logs des Apache Webservers einliest und diese
			modifizieren kann. So können alle Anfragen vervielfacht, Spitzenlasten verstärkt 
			oder über einen Bewertungsalgorithmus neue Anfragen eingefügt werden, welche die Charakteristik
			des Log-Verlaufs nicht verändern. Die servload Anwendung ist der eigentliche Benchmark, der
			das von servprep vorbereitete Log wieder einspielt. Dabei ist servprep in der Programmiersprache
			Lua und servload in C geschrieben. In der weiteren Entwicklung des Benchmarks wurde servprep in
			servload integriert. Dadurch konnte eine noch bessere Leistung erzielt und der
			Ressourcenverbrauch minimiert werden. Im Rahmen einer Semesterarbeit \cite{menski2012} wurde
			servload in sofern erweitert, dass es Logs vom BIND DNS-Server verarbeiten und DNS-Anfragen über
			UDP versenden kann. Somit ist es möglich, servload in dieser Arbeit als Benchmark einzusetzen.

		% section Benchmark }}}

		\section{Fazit} % {{{
		\label{sub:salbnet-fazit}

			In diesem Kapitel wurden die theoretischen Grundlagen und die bereits vorhandenen Metriken und
			Meldestrategien von salbnet erläutert. Basierend auf diesen Informationen wird im folgenden
			Kapitel ein Konzept dafür erarbeitet, wie sich salbnet auf DNS-Anfragen anpassen lässt. Außerdem wurde
			ein Benchmark vorgestellt, welcher dazu geeignet ist, die Erweiterung von salbnet auf seine
			Funktion zu testen und mit bestehenden Algorithmen zu vergleichen.

		% section Fazit }}}

	% chapter salbnet }}}

	\chapter{Konzept} % {{{
	\label{cha:konzept}

	In diesem Kapitel sollen mögliche Konzepte zur Erweiterung von salbnet für DNS-Anfragen
	vorgestellt werden. Danach wird eines der Konzepte ausgewählt und ein Entwurf zur Implementierung
	präsentiert. Die Implementierung des gewählten Entwurfs wird dann im Kapitel
	\ref{cha:implementierung} beschrieben.


		\section{Ansätze} % {{{
		\label{sec:ansaetze}

		Wie im Kapitel \ref{cha:salbnet} beschrieben, ist salbnet ursprünglich nur für TCP-Anwendungen
		konzipiert und für den Apache Webserver implementiert. Dazu wurde eine Metrik über die Backlog
		des TCP-Sockets des Webservers erstellt. Diese gibt konkret die Anzahl der Anfragen an, welche
		noch in der Backlog Platz finden würden. Das heißt, es sollten nicht mehr als diese Anzahl von
		Anfragen an den Server gestellt werden, sonst existiert die Gefahr, dass die Anfragen verworfen
		werden. Aufgabe dieses Kapitels ist es, verschiedene Ansätze zu beschreiben, welche eine ähnlich
		gute Metrik für UDP-Anfragen erzeugen.

		\subsection*{Last-Metrik} % {{{

		Wenn sich ein Server in einer Überlastsitutation befindet, erkennt man das oft daran, dass
		bestimmte Ressourcen aufgebraucht sind. So kann eine 100\%ige CPU- oder RAM-Auslastung aussagen
		sprechen, dass der Server an seine Grenzen geraten ist. Daher wäre eine mögliche Metrik die
		Sammlung entsprechender Last-Daten und eine anschließende Abschätzung bezüglich der verbleibenden
		Ressourcen.
		
		% subsection Last-Metrik }}}

		\subsection*{Verkehr-Metrik} % {{{

		Bei Servern, die einen Dienst für das Netzwerk bereitstellen, ist das Verhältnis der eingehenden
		und ausgehenden Nachrichten ebenfalls eine mögliche Metrik für die Lastsitutation des Servers.
		Ein Server, der alle eingehenden Nachrichten sofort beantworten kann, und somit die gleiche
		Anzahl an ausgehenden Nachrichten erzeugt, befindet sich in keiner Überlastsituation.  Hat der
		Server jedoch Probleme, eingehenden Anfragen direkt zu beantworten, befindet er sich unter einer
		gewissen Last, bis hin zur Überlast, bei der Anfragen verworfen werden müssen.  Somit wäre es
		möglich, die Anzahl der eingehenden Anfragen und abgearbeiteten (ausgehenden) Anfragen zu
		protokollieren. Wenn die Differenz nahe 0 ist, geht man von keiner hohen Last aus.  Wächst die
		Differenz jedoch allmählich, befindet sich der Server unter steigender Last.

		% subsection Verkehr-Metrik }}}

		\subsection*{Receive-Queue-Metrik} % {{{

		Ein Ansatz ähnlich des TCP-Backlog-Ansatzes \cite{zinke2007,scsczile2008} wäre das Überwachen
		der Receive-Queue des UDP-Sockets des DNS-Servers. Anders als bei dem TCP-Fall werden aber in
		der Receive-Queue keine Verbindungen verwaltet, sondern Pakete unterschiedlicher Größe
		gespeichert. Das heißt, die Receive-Queue hat eine maximale Größe und ist bis zu einem
		bestimmten Punkt gefüllt. Dies sagt aber nichts über die Anzahl an Paketen in der Receive-Queue
		aus.  Trotzdem könnte eine Metrik darin bestehen, den verbleibenden Platz in der Receive-Queue
		zu bestimmen, um daraus eine Abschätzung zu treffen, wieviele weitere Anfragen aufgenommen
		werden könnten.

		% subsection Receive-Queue-Metrik }}}
		
		% section Ansätze }}}

		\section{Auswahl} % {{{
		\label{sec:auswahl}

		Nachdem im Abschnitte \ref{sec:ansaetze} mehrere Ansätze vorgestellt wurden, sollen diese nun
		bewertet werden. Danach wird ein Ansatz ausgewählt, welcher dann weiter verfeinert wird. Die
		Bewertung der Ansätze erfolgt nach den im Kapitel \ref{cha:salbnet} aufgeführten Anforderungen
		an eine Metrik, welche waren:

		\begin{itemize}
			\item Sie sollte einfach zu berechnen sein.
			\item Sie sollte die aktuell zur Verfügung stehenden Ressourcen des Servers angeben.
			\item Sie sollte applikationsunabhängig sein.
			\item Sie sollte auch für Dienste mit mehreren Tausend Clients skalieren.
		\end{itemize}

		\subsection*{Last-Metrik} % {{{

		Die Last-Metrik ist applikationsunabhängig und wird auch nicht durch die Anzahl der Clients
		beeinflusst. Allerdings ist es schwer, aus Faktoren wie der CPU-Last, RAM-Auslastung, Load etc.
		eines Servers Aussagen über die aktuelle Lastsituation des Servers zu treffen. In
		\cite{kunz1991} wurden verschiedene Last-Metriken untersucht. Dabei wurde gezeigt, dass einfache
		Metriken bessere Ergebnisse lieferten als mehrdimensionale, über mehrere Indizes. Die
		besten Ergebnisse wurden mit einer Metrik, die sich ausschließlich auf die Länge der
		CPU-Warteschlange bezieht, erzielt. Bei dieser Untersuchung wurde ausschließlich ein homogenes System
		betrachtet. Um diese Metriken auf ein heterogenes System anzuwenden, muss eine Normierung
		vorgenommen werden, damit die System und deren Metriken vergleichbar werden. Dazu schlägt
		\cite{lansch1994} die Formel \ref{eq:load} vor, bei der $\alpha$ eine Architekturkonstante für
		die Grundgeschwindigkeit und \textit{load} die aktuelle Last des Systems ist.

		\begin{equation}
			I := \alpha \cdot (1 + \text{load})
			\label{eq:load}
		\end{equation}

		Selbst, wenn eine normierte Metrik genutzt wird, existiert trotzdem noch das Problem, dass mit
		dieser Metrik eine Abschätzung über die noch möglichen Ressourcen, in Form von weiteren
		Anfragen, nicht einfach ist. Zum Beispiel lässt sich kaum aus der Länge der CPU-Warteschlange
		abschätzen, wieviel weitere Anfragen der Server noch empfangen kann, bevor er in eine
		Überlastsituation gerät. Daher ist die Metrik schwer zu berechnen, bzw. führt nur zu sehr
		ungenauen Abschätzungen und erfüllt damit nicht die Anforderungen.

		% subsection Last-Metrik }}}

		\subsection*{Verkehr-Metrik} % {{{

		Die Verkehr-Metrik ist ebenfalls applikationsunabhängig und einfach zu berechnen, da nur die
		Differenz der eingehenden und ausgehenden Anfragen ermittelt werden muss. Ist diese Differenz
		negativ, ist der Server schneller im Abarbeiten als dass neue Anfragen eintreffen. Ist sie gleich
		0, ist der Server in einem Gleichgewicht, in dem er alle eintreffenden Anfragen sofort
		beantworten kann.  Ist die Differenz positiv, stauen sich Anfragen beim Server. Steigt diese
		Differenz immer weiter an, kann davon ausgegangen werden, dass der Server Probleme hat, weitere
		Anfragen abzuarbeiten. Die Probleme dieser Metrik sind jedoch die Abbildung der Differenz auf
		die verfügbaren Ressourcen des Servers. Das bedeutet, es ist schwer, anhand der Differenz auf die
		noch möglichen Anfragen zu schließen, welche der Server zusätzlich abarbeiten könnte, ohne dass
		eine Anfrage verworfen wird. Es könnte eine Abschätzung der maximal möglichen Anfragen pro
		Zeitintervall genutzt werden. Diese vernachlässigt allerdings die UDP-Eigenschaft, dass in der
		Receive-Queue keine Anfragen sondern Pakete gespeichert werden. Das heißt, die maximale
		Anzahl an Anfragen pro Zeitintervall hängt von den einzelnen Paketgrößen ab. Außerdem
		kann davon ausgegangen werden, dass ein Protokollieren aller ein- und ausgehenden Anfragen nicht
		gut skaliert. So würde bei einer hohen Anzahl von Anfragen an den Server die Überwachung
		eine zusätzlich Last erzeugen, welche den Server extra belastet. Daher erfüllt auch diese Metrik
		ebenfalls nicht die Anforderungen.

		% subsection Verkehr-Metrik }}}

		\subsection*{Receive-Queue-Metrik} % {{{

		Die Receive-Queue-Metrik ist, wie auch die beiden vorherigen, applikationsunabhängig, da sie
		sich auf die Receive-Queue des UDP-Sockets bezieht. Sie erzeugt auch keine zusätzliche Last,
		wenn die Client-Anzahl steigt. Die Berechnung ist etwas schwieriger als bei der
		TCP-Backlog-Metrik, da, wie bereits erwähnt, keine Verbindungen gespeichert werden, sondern
		Pakete. Allerdings kann aus dem verbleibenden Platz in der Receive-Queue und einer Abschätzung
		über die durchschnittliche Paketgröße eine Vorhersage getroffen werden, welche zusätzliche
		Anzahl an Anfragen der Server verarbeiten könnte. Damit erfüllt die Metrik auch die
		Anforderungen der einfachen Berechnung und der Angabe von verfügbaren Ressourcen.

		% subsection Receive-Queue-Metrik }}}

		Wie bereits durch die vorhergehenden Bewertungen zu erkennen ist, hat nur die Receive-Queue-Metrik die
		Anforderung aus \cite{scsczile2008} an eine Metrik für salbnet erfüllt. Diese wird in Abschnitt
		\ref{sec:entwurf} genauer beschrieben und in Kapitel \ref{cha:implementierung} wird die
		konkrete Implementierung in salbnet erläutert.

		% section Auswahl }}}

		\section{Entwurf} % {{{ 
		\label{sec:entwurf}

		Die ausgewählte Receive-Queue-Metrik wird in diesem Abschnitt im Detail beschrieben.
		Dazu werden alle nötigen Messwerte identifiziert und eine Formel entwickelt, mit der aus den
		Messwerten die Credits des Servers berechnet werden können. Die Metrik soll den Server anhand der
		UDP-Receive-Queue des DNS-Servers bewerten. Daher werden folgende Messwerte benötigt: $~$\\
		
		\begin{tabular}{rl}
			$q_{max}$		  & die maximale Kapazität der UDP-Receive-Queue\\
			$q_{current}$ &	der aktuell belegte Speicherplatz in der UDP-Receive-Queue\\
			$p_{median}$  &	der Median der Paketgrößen in der UDP-Receive-Queue\\
										& \\
		\end{tabular}

		Mit Hilfe dieser Messwerte kann eine Abschätzung getroffen werden, wieviel weitere Pakete
		$p_{additional}$ (Anfragen) noch angenommen werden können, ohne dass eine Anfrage verworfen wird.
		Die Abschätzung ist in der Formel \ref{eq:additional} dargestellt. 

		\begin{equation}
			p_{additional} = \frac{q_{max} - q_{current}}{p_{median}}
			\label{eq:additional}
		\end{equation}

		Diese Abschätzung gibt eine Bewertung für den aktuellen Zeitpunkt an. Wie im Kapitel
		\ref{cha:salbnet} beschrieben, existieren verschiedene Meldestrategien in salbnet. Bei den
		optimierten Strategien Dynamic Report und Dynamic Pressure Relieve (siehe Abschnitt
		\ref{sub:meldestrategien}) werden die Credits in dynamisch berechneten Intervallen gemeldet. Das
		heißt, zwischen zwei Credit-Meldungen kann sich die Receive-Queue stark verändern. Die
		Abschätzung enthält aber keinen Faktor, welcher die Tendenz seit der letzten Credit-Berechnung
		darstellt. Eine Lösung wäre, bei jeder Abarbeitung einer Anfrage Credits zu berechnen und diese
		zu speichern. Dadurch könnte der Verlauf mit in der Abschätzung genutzt werden. Allerdings würde
		dies besonders bei einer hohen Last auf dem Server zu einem zusätzlichen Aufwand führen, welcher
		die Leistung des Servers negativ beeinflussen kann. Als Kompromiss, zwischen ständiger
		Credit-Berechnung und Vernachlässigung des Verlaufs gibt es die Möglichkeit, nur auf
		Überlastsituationen zu reagieren. Das heißt, es wird überprüft, ob der Server sich zwischen zwei
		Credit-Meldungen in einer Überlastsitutation befunden hat. Das erkennt man zum Beispiel daran,
		ob Pakete verworfen worden. Dies geschieht, wenn die maximale Kapazität der Receive-Queue
		erreicht wurde. Sind seit der letzten Credit-Meldung Anfragen verworfen wurden, sollte das bei
		der Berechnung der aktuellen Credits beachtet werden.  Dabei ist die Frage, wie das Verwerfen
		von Anfragen bewertet wird. In \cite{scsczile2008} wird eine nicht beantwortete Anfrage als ein
		verlorener Nutzer und potential als ein verlorener Kunde bewertet. Dabei ist zu beachten, dass
		die Aussage für einen HTTP-Anwendungsfall getroffen wurde.  Allerdings ist es auch bei DNS ein
		sinnvolles Ziel, die verworfenen Anfragen zu minimieren. Daher sollen dem Lastverteiler 0 Credits
		gemeldet werden, wenn seit der letzten Credits-Meldung Anfragen verworfen wurden. Somit kann der
		Server die aktuellen Anfragen abarbeiten ohne weitere Anfragen zu erhalten. Dadurch wird sich
		die Last des Servers vermutlich bis zur nächsten Credit-Meldung verringern. Anschließend können
		wieder Credits entsprechend der aktuellen Situation gemeldet werden.

		Wird nun die Anzahl der verworfenen Anfrage seit der letzten Credit-Meldung $p_{drop}$
		in der Abschätzung beachtet, ergibt sich Formel \ref{eq:credits} zur Berechnung der Credits des
		DNS-Servers.

		\begin{equation}
			Credits = \begin{cases}0 & p_{drop}>0\\ \frac{\displaystyle q_{max} - q_{current}}{\displaystyle p_{median}}
			  & \text{sonst}\end{cases}
			\label{eq:credits}
		\end{equation}


		% section Entwurf }}}

		\section{Fazit} % {{{
		\label{sec:konzept-fazit}
		
		% section Fazit }}}

	% chapter Konzept }}}

	\chapter{Implementierung} % {{{
	\label{cha:implementierung}

	Dieses Kapitel beschreibt die Implementierung der im Kapitel \ref{cha:konzept} entworfenen Metrik.
	Dazu wird in Abschnitt \ref{sec:anforderungen} die Frage geklärt, wie die in Abschnitt
	\ref{sec:entwurf} definierte Receive-Queue-Metrik berechnet wird. Das heißt, es wird gezeigt, welche
	Möglichkeiten existieren, die benötigten Messwerte zu erhalten. Außerdem werden die erhaltenen
	Messwerte bewertet und es wird auf eventuelle Probleme oder Ungenauigkeiten dieser Werte
	eingegangen. Nachdem feststeht, wie alle Messwerte ermittelt werden können, wird in Abschnitt
	\ref{sec:erweiterung} auf die konkrete Implementierung in salbnet eingegangen. Im Kapitel
	\ref{cha:messungen} werden Messungen vorgestellt, welche die erweiterte Version von salbnet mit
	einem Standard-Algorithmus vergleichen.

		\section{Anforderungen} %  {{{
		\label{sec:anforderungen}

		In diesem Abschnitt sollen Möglichkeiten diskutiert werden, mit deren Hilfe die einzelnen
		Messwerte, für die in Abschnitt \ref{sec:entwurf} beschriebene Metrik, ermittelt werden können. Es
		werden dabei folgende Werte benötigt: $~$\\
		
		\begin{tabular}{rl}
			$q_{max}$		  & die maximale Kapazität der UDP-Receive-Queue\\
			$q_{current}$ &	der aktuell belegte Speicherplatz in der UDP-Receive-Queue\\
			$p_{median}$  &	der Median der Paketgrößen in der UDP-Receive-Queue\\
			$p_{drop}$    &	die Anzahl der verworfenen Pakete seit der letzten Credit-Meldung\\
										& \\
		\end{tabular}

		\subsection*{Maximale Kapazität der UDP-Receive-Queue} % {{{

		\lstinputlisting[float,lastline=4,caption={Standard-Queue-Größe für UDP-Sockets},label=lst:default-queue]{listings/proc-rmem.txt}
		\lstinputlisting[float,firstline=6,caption={Erhöhen der Queue-Größe für UDP-Sockets},label=lst:test-queue]{listings/proc-rmem.txt}

		Die maximale Kapazität $q_{max}$ der UDP-Receive-Queue für jeden UDP-Socket wird im Kernel festgelegt. Der
		aktuelle Wert kann über das \texttt{/proc}-Dateisystem festgestellt werden. Beim
		\texttt{/proc}-Dateisystem \cite{benvenuti2005} handelt es sich um ein virtuelles Dateisystem,
		welches kernelinterne Datenstrukturen, in der Form von Dateien, in den Userspace exportiert. In
		Listing \ref{lst:default-queue} werden die Werte der beiden entsprechenden Dateien im
		\texttt{/proc}-Dateisystem ausgelesen, welche die Standard- und maximale Größe der
		UDP-Receive-Queue eines UDP-Sockets bestimmen. Um diese Werte zu verändern, kann das Programm
		\texttt{sysctl} genutzt werden, welches Variablen im \texttt{/proc/sys}-Verzeichnis bearbeiten
		kann, oder es kann direkt in die entsprechenden Dateien ein neuer Wert geschrieben werden. Die
		Verwendung von \texttt{sysctl} ist in Listing \ref{lst:test-queue} zu sehen.

		% subsection Maximale Kapazität der UDP-Receive-Queue }}}

		\subsection*{Aktuell belegter Speicherplatz in der UDP-Receive-Queue} % {{{

		Der aktuell belegte Speicherplatz $q_{current}$ in der UDP-Receive-Queue eines UDP-Sockets kann
		ebenfalls über das \texttt{/proc}-Dateisystem ermittelt werden. Diese Information ist in der
		Datei \texttt{/proc/net/udp} enthalten. In Listing \ref{lst:proc-udp} ist ein Auszug aus dieser
		Datei zu sehen. Jede Zeile steht für einen Socket. Dabei ist \texttt{sl} der Hash-Slot des
		Sockets im Kernel. Die \texttt{local\_address} ist die lokale Adresse und Port des Sockets. Wenn
		der Socket direkt  mit einer entfernten Adresse verbunden ist, wird dies im Feld
		\texttt{rem\_address} angezeigt.  Der Status des Sockets ist in \texttt{st} vermerkt und
		\texttt{tx\_queue} bzw.  \texttt{rx\_queue} geben den Speicherverbrauch der UDP-Send-Queue bzw.
		UDP-Receive-Queue an. Es folgen noch weitere Werte, die in diesem Zusammenhang jedoch unwichtig
		sind. Somit kann durch diese Datei der belegte Speicherplatz der UDP-Receive-Queue für einen
		bestimmten UDP-Socket ermittelt werden.

		Der Nachteil vom \texttt{/proc}-Dateisystem ist, dass beim wiederholten Abfragen eines Wertes
		jedesmal eine Datei geöffnet, gelesen und geschlossen werden muss. Danach muss der Inhalt der
		Datei ausgewertet werden, um den gesuchten Wert zu finden. Einen möglichen Ersatz, vor allem bei
		der Netzwerkkonfiguration, bieten \texttt{Netlink}-Sockets \cite{rfc3549, gusowski2009}.  Durch
		sie ist es möglich, zwischen dem Kernel- und Userspace über einen Socket zu kommunizieren.  Dies
		hat einige Vorteile gegenüber dem \texttt{/proc}-Dateisystem, da nur initial ein Socket erstellt
		werden muss, welcher danach weiterverwendet werden kann. Es können aber auch Werte gezielt
		abgefragt werden, ohne sie aus einer Textrepräsentation extrahieren zu müssen. In der
		Implementation von salbnet \cite{zinke2012,salbnet} wird ein \texttt{Netlink}-Socket genutzt, um
		Informationen bezüglich der TCP-Backlog zu sammeln. Daher war es naheliegend, ein ähnliches
		Vorgehen auch für die UDP-Receive-Queue zu evaluieren. Allerdings wurde das Kernelmodul des
		salbnet Schedulers für die Kernel-Version 2.6.18 entwickelt und kann nicht ohne Anpassungen mit
		neueren Versionen genutzt werden. Eine Erweiterung der \texttt{Netlink}-Sockets um ein UDP-Modul
		\cite{udpnetlink} wurde jedoch erst im Linux-Kernel 3.3 aufgenommen. Somit ist die Verwendung
		eines \texttt{Netlink}-Sockets, zur Abfrage des aktuell belegten Speicherplatzes in der
		UDP-Receive-Queue, in salbnet nicht möglich.

		\lstinputlisting[float,caption={/proc/net/udp	Informationen zu UDP-Sockets},label=lst:proc-udp]{listings/proc-udp.txt}

		% subsection Aktuell belegter Speicherplatz in der UDP-Receive-Queue }}}

		\subsection*{Median der Paketgrößen in der UDP-Receive-Queue} % {{{

		Um den Median der Paketgrößen $q_{median}$ in der UDP-Receive-Queue bestimmen zu können, reicht
		die Information über den belegten Speicherplatz in der UDP-Receive-Queue nicht aus, denn es ist
		nicht ermittelbar, wieviele Pakete aktuell in der Receive-Queue gespeichert sind. Holt man jedoch
		ein Paket aus der Receive-Queue ab, wird die Größe des Pakets zurückgeliefert. In Listing
		\ref{lst:strace} wurde das Programm \texttt{strace} genutzt, um die Systemcalls des BIND
		DNS-Servers nachvollziehen zu können. Dazu wurde in Zeile 1 zuerst die Process ID (PID) des BIND Servers
		(named) ermittelt. Dann wurde in Zeile 3 strace für diese PID gestartet und die Ausgaben aller
		Kindprozesse in einzelnen Dateien gespeichert. Danach ist in der Ausgabe-Datei des Kindprozesses,
		welcher die Anfrage verarbeitet hat, in Zeile 12 zu erkennen, dass ein Paket vom BIND mit Hilfe
		der Funktion \texttt{recvmsg} aus der Receive-Queue abgeholt wird. Der Rückgabewert der Funktion
		gibt die Größe des Paketes in Byte an. In diesem Beispiel war das DNS-Paket 45 Byte groß. Somit
		ist es möglich, den Funktions-Aufruf des BIND abzufangen und die Paketgröße zu ermitteln.

		\lstinputlisting[float,lastline=12,caption={strace-Ausgabe für den BIND-Server bei der Abfrage von \texttt{www.haiti.cs.uni-potsdam.de}},label=lst:strace]{listings/strace.txt}

		Jedoch ist die Größe des DNS-Paketes, wie sie von der Funktion \texttt{recvmsg}
		zurückgegeben wird, nicht der komplette Speicherbedarf des Pakets in der Receive-Queue. Im
		Linux-Kernel werden die Pakete, welche in einem Socket-Buffer liegen, mit Hilfe der
		Datenstruktur \texttt{sk\_buff} verwaltet. Im Anhang \ref{cha:skbuff} ist das Listing
		\ref{lst:skbuff} der \texttt{sk\_buff}-Datenstruktur für den Linux-Kernels 2.6.18 angeführt. Es ist
		zu erkennen, dass eine große Anzahl an zusätzlichen Informationen in der Datenstruktur neben
		dem eigentlichen DNS-Paket gespeichert wird. Das Feld \texttt{truesize} in der Datenstruktur,
		enthält die vollständige Größe des Pakets in der Receive-Queue. Allerdings ist ein Zugriff auf
		diesen Wert aus dem Userspace nicht möglich.

		\begin{table}
			\centering
			\begin{tabular}{|c|c|}\hline
				Receive-Queue & recvmsg \\\hline\hline
				376 & 12-64	\\
				504 & 65-192	\\
				632 & 193-320	\\
				760 & 321-448 \\
				888 &	449-576\\\hline
			\end{tabular}
			\caption{Experimentelle Bestimmung des Zusammenhangs zwischen der Größe des DNS-Pakets und dem
			Speicherplatz in der Receive-Queue (Angaben in Byte)}
			\label{tab:recvmsg}
		\end{table}

		Um auf eine Anpassung im Kernelspace zu verzichten, wurde der Zusammenhang zwischen der Größe
		des DNS-Pakets und dem Speicherplatz in der Receive-Queue experimentell ermittelt. Hierzu
		wurden DNS-Pakete mit steigender Größe an einen BIND DNS-Server gesendet. Nach jedem Paket wurde
		der belegte Speicherplatz der Receive-Queue und anschließend die Größe des Pakets mit
		der Funktion \texttt{recvmsg} ermittelt. Diese Versuche wurden mehrmals durchgeführt, dadurch
		konnte bestätigt werden, dass das Verhalten deterministisch ist und es einen direkt Zusammenhang
		zwischen der Größe des DNS-Pakets und dem benötigten Speicher in der Receive-Queue gibt. Der
		Header eines DNS-Pakets ist 12 Byte groß und die maximale Größe eines DNS-Pakets beträgt 512 Byte
		\cite{rfc1035}, deshalb wurde das Experiment mit Paketen der Größe 12-576 Byte durchgeführt. Das
		Ergebnis ist in Tabelle \ref{tab:recvmsg} aufgeführt. Es zeigt sich, dass 376 Byte die minimale
		Größe eines DNS-Pakets in der Receive-Queue ist. Überschreitet das DNS-Pakete eine Größe von 64
		Byte wächst der Speicherbedarf um 128 Byte. Diese 128 Byte stehen ausschließlich für den Inhalt
		des DNS-Pakets zur Verfügung, da alle zusätzlichen Daten der \texttt{sk\_buff}-Datenstruktur
		bereits enthalten sind. Erhöht sich die Größe des DNS-Pakets ebenfalls um 128 Byte und
		überschreitet 192 Byte, wächst der Speicherbedarf wiederum um 128 Byte an. Zusammenfassend
		kann man sagen, dass ab einer Paketgröße von 64 Byte alle 128 Byte eine Erhöhung des
		Speicherbedarfs um ebenfalls 128 Byte eintritt. Daher ist es möglich, die Formel \ref{eq:recvmsg}
		herzuleiten, mit der man anhand der Größe des DNS-Pakets (ermittelt durch \texttt{recvmsg}) den
		Speicherbedarf in der Receive-Queue berechnet. Es ist zu beachten, dass diese Formel nicht
		allgemeingültig ist und speziell für die Kernel-Version 2.6.18 ermittelt wurde. Für andere
		Kernel-Versionen kann sich ein veränderter Zusammenhang ergeben und somit auch eine andere
		Formel gelten.

		\begin{equation}
			\text{Receive-Queue} = \lfloor (\text{recvmsg} + 63) / 128 \rfloor \cdot 128 + 376
			\label{eq:recvmsg}
		\end{equation}


		% subsection Median der Paketgrößen in der UDP-Receive-Queue }}}

		\subsection*{Verworfene Anfragen seit der letzten Credit-Meldung} % {{{

		In neueren Kernel-Versionen werden die Drops (Anzahl der verworfenen Anfragen) pro UDP-Socket
		ebenfalls in der Datei \texttt{/proc/net/udp} aufgeführt. Für die verwendete Kernel-Version
		2.6.18 gilt dies leider noch nicht. Daher wurde auf die Datei \texttt{/proc/net/snmp}
		zurückgegriffen. Wie in Listing \ref{lst:proc-snmp} zu sehen ist, wird in dieser Datei die Anzahl
		der systemweiten Drops als \texttt{InErrors} angegeben. Da in dieser Arbeit davon ausgegangen
		wird, dass es sich um ein DNS-Cluster handelt, welches ausschließlich zum Beantworten von
		DNS-Anfragen genutzt wird, kann angenommen werden, dass der DNS-Server den Hauptanteil
		des UDP-Verkehrs erzeugt. Somit sind Drops, auch wenn diese nur systemweit angegeben werden,
		kritisch für den DNS-Server. Zur Berechnung der Drops seit der letzten Credit-Meldung $q_{drop}$
		muss die Differenz zum vorherigen Wert berechnet werden.
		
		\lstinputlisting[float,caption={/proc/net/snmp Protokoll-Information für SNMP-Programme},label=lst:proc-snmp]{listings/proc-snmp.txt}

		% subsection Verworfene Anfragen seit der letzten Credit-Meldung }}}


		% section Anforderungen }}}

		\section{Erweiterung von salbnet} % {{{
		\label{sec:erweiterung}

		Dieser Abschnitt stellt die konkrete Implementation, der in Abschnitt \ref{sub:entwurf}
		entworfenen Receive-Queue-Metrik, vor. Die dafür benötigten Werte werden wie in Abschnitt
		\ref{sec:anforderungen} beschrieben ermittelt. Für die Ermittlung von $q_{max}$, $q_{current}$
		und $q_{drop}$ muss die Komponente von salbd erweitert werden, welche für das Sammeln der
		Messwerte zuständig ist. Das geschieht in der Datei \texttt{linux/client\_metric.c} von salbd.
		Um den Median der Paketgrößen zu berechnen muss, wie beschrieben, der Funktionsaufruf
		\texttt{recvmsg} vom BIND Server abgefangen werden. Dazu wird in der Datei \texttt{nethook.c}
		der libnethook-Bibliothek eine neue Funktion benötigt, welche die \texttt{recvmsg}-Funktion der
		\texttt{libc} überlagert. Abschließend müssen aus den gesammelten Werten die Credits berechnet
		werden. Diese Berechnung findet in der Datei \texttt{client\_report.c} in salbd statt. Im
		Folgenden werden die genauen Änderungen am Quellcode beschrieben.

			\subsection*{Maximale Kapazität der UDP-Receive-Queue} % {{{

			Bei der Erstellung eines UDP-Sockets erhält die Recevie-Queue des Sockets eine Standardgröße.
			Diese Größe ist, wie bereits in Abschnitt \ref{sec:anforderungen} gezeigt, in der Datei
			\texttt{rmem\_default} im Verzeichniss \texttt{/proc/sys/net/core/} zu finden. In Listing
			\ref{lst:metric-max-udp} wird in Zeile 2 überprüft, ob bereits die maximale Kapazität der
			UDP-Recevie-Queue ermittelt wurde.  Es wird davon ausgegangen, dass während des Betriebs von
			salbnet nicht die Queue-Größen verändert werden. Somit ist ein einmaliges Auslesen der Größe
			ausreichend. Wurde die Größe noch nicht bestimmt, wird in Zeile 3 die entsprechende Datei im
			\texttt{/proc}-Verzeichnis geöffnet und in Zeile 9 gelesen. Dabei wird der gesuchte Wert
			direkt in die \texttt{maximum}-Variable der Metrik geschrieben. Wenn keine Fehler aufgetreten
			sind wird die Datei in Zeile 14 geschlossen und die Ermittlung der maximalen Kapazität der
			UDP-Receive-Queue ist abgeschlossen.

			\lstinputlisting[float,language=C,firstline=42,lastline=60,caption={Bestimmung der maximalen Kapzität der UDP-Receive-Queue (\texttt{linux/client\_metric.c} aus salbd)},label=lst:metric-max-udp]{listings/client_metric.c}

			% subsection Maximale Kapazität der UDP-Receive-Queue }}}

			\subsection*{Aktuell belegter Speicherplatz in der UDP-Receive-Queue} % {{{

			\lstinputlisting[float,lastline=39,caption={Bestimmung des belegten Speicherplatz in der UDP-Receive-Queue (\texttt{linux/client\_metric.c} aus salbd)},label=lst:metric-udp-proc]{listings/client_metric.c}

			% subsection Aktuell belegter Speicherplatz in der UDP-Receive-Queue }}}

			\subsection*{Median der Paketgrößen in der UDP-Receive-Queue} % {{{

			\lstinputlisting[float,caption={Datenstruktur \texttt{nethook\_data} aus \texttt{nethook.h} der libnethook},label=lst:nethookh]{listings/nethook.h}

			\lstinputlisting[float,caption={Auszüge der Funktion \texttt{recvmsg} aus \texttt{nethook.c} der libnethook},label=lst:nethookc]{listings/nethook.c}

			% subsection Median der Paketgrößen in der UDP-Receive-Queue }}}

			\subsection*{Verworfene Anfragen seit der letzten Credit-Meldung} % {{{

			\lstinputlisting[float,firstline=63,caption={Bestimmung der verworfenen Anfragen (\texttt{linux/client\_metric.c} aus salbd)},label=lst:metric-drops-udp]{listings/client_metric.c}


			% subsection Verworfene Anfragen seit der letzten Credit-Meldung }}}

			\subsection*{Bestimmung der Hard- und Soft-Credits} % {{{
			
			\lstinputlisting[float,lastline=38,caption={Bestimmung der Hard- und Soft-Credits (\texttt{client\_report.c} aus salbd)},label=lst:metric-udp-proc]{listings/client_report.c}
			
			% subsection Bestimmung der Hard- und Soft-Credits }}}

			\subsection*{Bestimmung der Auslastung der Receive-Queue} % {{{

			\lstinputlisting[float,firstline=40,caption={Bestimmung der maximalen und aktuellen Auslastung der UDP-Receive-Queue	(\texttt{client\_report.c} aus salbd)},label=lst:metric-drops-udp]{listings/client_report.c}

			% subsection Bestimmung der Auslastung der Receive-Queue }}}

		% section Erweiterung von salbnet }}}

		\section{Fazit} % {{{
		\label{sec:implementierung-fazit}
		
		% section Fazit }}}

	% chapter Implementierung }}}

	\chapter{Messungen} % {{{
	\label{cha:messungen}

		In diesem Kapitel werden die durchgeführten Funktionsmessungen dargestellt. Es soll gezeigt
		werden, dass die Implementation funktionstüchtig und zum Verteilen von DNS-Anfragen geeignet
		ist. Außerdem wurde ein Vergleich zum Round-Robin-Verfahren des LVS durchgeführt. Dieser soll
		zeigen, dass das salbnet Verfahren eine bessere Verteilung der Last auf den Backend-Servern
		erzeugt. In Abschnitt \ref{sec:messumgebung} wird der Aufbau der Messumgebung erläutert. Danach
		wird in Abschnitt \ref{sec:messplan} der Ablauf der Messung beschrieben und in Abschnitt
		\ref{sec:auswertung} folgt eine Auswertung.

		\section{Messumgebung} % {{{
		\label{sec:messumgebung}

		Die Messumgebung soll ein realistisches Lastszenario wiederspiegeln. Dazu wurden aus dem
		IB-Cluster des Instituts für Informatik 4 Rechner ausgewählt (ib1, ib4, ib6, ib8). Die
		relevanten technischen Merkmale dieser Server sind in der Tabelle \ref{tab:netzwerkknoten}
		aufgeführt. Bei den	Maschinen ib1 und ib4 handelt es sich um baugleiche Server. Die Maschine ib6
		ist die älteste und auch schwächste Maschine, da sie nur einen Single-Core Prozessor besitzt.
		Der Aufbau soll ein DNS-Server-Cluster aus 3 Nameservern und einem Lastverteiler simulieren.
		Dabei ist die Maschine ib1 der Lastverteiler und die 3 heterogenen Maschinen ib4, ib6 und ib8
		stellen jeweils einen BIND Nameserver bereit.	Auf allen ib-Maschinen ist ein CentOS 5.7 mit dem
		Kernel 2.6.18-274.12.1.el5 installiert.  Auf den Maschinen ib4, ib6 und ib8 ist BIND in der
		Version 9.3.6-20.P1 installiert. Auf den BIND Servern wurde die
		\texttt{haiti.cs.uni-potsdam.de}-Domain mit dem Stand vom 26. Oktober 2011 eingerichtet. Die
		Anzahl der einzelnen Resource Record Typen ist in Tabelle \ref{tab:rr-domain} aufgeführt. Die
		Konfiguration des BIND Servers ist in Anhang \ref{lst:bind-conf} zu finden. Um eine höhere
		Auslastung der Nameserver zu erhalten wurde die UDP-Receive-Queue auf den Maschinen ib1, ib4,
		ib6 und ib8 auf \unit[24]{MB} erhöht. Dadurch wird verhindert, das DNS-Anfragen verworfen
		werden, weil die Receive-Queue voll ist, obwohl der Nameserver diese noch beantworten könnte.
		
		Zum Absenden der DNS-Anfragen wird die Maschine node015 aus dem Leibniz-Cluster des Institut für
		Informatik genutzt. Sie besitzt einen Quad-Core Prozessor und 12 GB RAM (siehe Tabelle
		\ref{tab:netzwerkknoten}). Auf ihr wird der servload Benchmark ausgeführt, da dieser eine
		ausreichend hohe Last erzeugen muss um die 3 Nameserver zu belasten ist diese leistungsstarke
		Maschine notwendig.

		\begin{table}
			\centering
			\begin{tabular}{|c|c|c|c|c|l|}\hline
				Node & CPU &  Taktung & Kerne & RAM & Funktion \\\hline\hline
				node015 & Intel Xeon E5520 & \unit[2,27]{GHz} & 1 $\times$ 4 & \unit[12]{GB} & servload\\
				ib1 & AMD Opteron 244 & \unit[1,8]{GHz} & 2 $\times$ 1 & \unit[4]{GB} & LVS, salbd (Server)\\
				ib4 & AMD Opteron 244 & \unit[1,8]{GHz} & 2 $\times$ 1 & \unit[4]{GB} & BIND, salbd (Client)\\
				ib6 & Intel Pentium 4 & \unit[2,8]{GHz} & 1 $\times$ 1 & \unit[4]{GB} & BIND, salbd (Client)\\
				ib8 & Intel Xeon 3040 & \unit[1,86]{GHz} & 1 $\times$ 2 & \unit[4]{GB} & BIND, salbd (Client)\\\hline
			\end{tabular}
			\caption{Technische Daten der verwendeten Netzwerkknoten}
			\label{tab:netzwerkknoten}
		\end{table}

		\begin{table}
			\centering
			\begin{tabular}{|c|c|}\hline
				Resource Record & Anzahl \\\hline\hline
				A & 107 \\
				NS & 4 \\
				MX & 2 \\
				CNAME & 2 \\
				PTR & 106 \\
				SOA & 1 \\\hline
			\end{tabular}
			\caption{Anzahl der Resource Record Typen in der \texttt{haiti.cs.uni-potsdam.de}-Domain}
			\label{tab:rr-domain}
		\end{table}
			
		% section Messumgebung }}}

		\section{Messplan} % {{{
		\label{sec:messplan}

		Zur Durchführung der  Messung wurde ein anonymisiertes Log des authorativen Nameservers der
		\texttt{haiti.cs.uni-potsdam.de}-Domain verwendet. Das Log enthält den Zeitraum von 10:41 am 27.
		September 2011 bis 6:25 am 2. Oktober 2011. Über den Zeitraum von 6944 Minuten enthält das Log
		1003149 Anfragen, welche hauptsächlich Anfragen nach \texttt{A} und \texttt{AAAA} Resource
		Records sind (siehe Tabelle \ref{tab:log}). Es enthält 1026 Sessions, wobei eine Session als
		alle Anfragen eines einzelnen Users definiert ist. Dadurch ergeben sich die Verhältnisse von
		\unit[9,33]{\nicefrac{Anfragen}{Sekunde}} und \unit[977,63]{\nicefrac{Anfragen}{Session}}. Diese
		Last würde keinen der 3 Nameserver auslasten. Um eine höhere Last erzeugen zu können wurde ein
		Zeitabschnitt um das Maximum des Logs ausgewählt. Dieser wurde dann durch die
		\texttt{multiply}-Methode des servload Benchmarks verstärkt um eine ausreichend hohe Last zu
		erzeugen. Als Zeitraum wurden 5 Minuten von 5:55 bis 6:00 am 29. September 2011 gewählt, in
		diesem liegt das Maximum mit \unit[204]{\nicefrac{Anfragen}{Sekunde}}. In diesem Intervall
		werden 22.594 Anfragen von 33 Sessions gestellt. Der sehr kurze Zeitraum von 5 Minuten war
		notwendig, um durch eine hohe Verstärkung das Nameserver-Cluster auszulasten. Wäre ein
		längerer Zeitraum gewählt worden, hätte der Arbeitspeicher des node015 Rechners nicht
		ausgereicht um das komplette verstärkte Log vorzuhalten.
		
		Mit dem ausgewählten Intervall wurden jeweils 3 Messungen mit unterschiedlichen
		Multiplikationsfaktoren durchgeführt. Es wurden die Faktoren 400, 800 und 1600 gewählt um hohe
		Lastsitutationen zu erzeugen. Die daraus resultierende Anzahl an Anfragen, Sessions und
		\nicefrac{Anfragen}{Sekunde} sind in der Tabelle \ref{tab:multiply} zu sehen. Eine höhere
		Verstärkung war wiederum nicht möglich ohne an die Grenzen des Arbeitsspeicher zu stoßen.

		Verglichen wurden der gewichtete Round-Robin-Algorithmus des LVS mit dem dynamic pressure
		relieve salbd Algorithmus. Die Gewichte für den Round-Robin Algorithmus wurden mit Hilfe des
		Benchmarks UnixBench \cite{unixbench} bestimmt. Dieser
		Benchmark führt verschieden Tests aus und ermitteln eine Gesamtbewertung des Systems. Die
		Messergebnisse für die Machinen ib4, ib6 und ib8 sind im Anhang \ref{lst:unixbench} zu sehen. Es
		wurden die Werte 788, 623 und 1180 für die ib4, ib6 und ib8 bestimmt. Diese Gewichte haben sich
		bereits in anderen Messungen als sinnvoll für den Round-Robin-Algorithmus erwiesen, deshalb
		wurden bei diesen Messungen wiederum auf sie zurückgegriffen. Bei dem salbd Algorithmus müssen
		keine Gewichte angegeben werden. Allerdings muss eine Meldestrategie gewählt werden. Messungen
		in einem TCP-Szenario haben den Algorithmus dynamic pressure relieve als eine der besten
		Meldestrategien ermittelt. Deshalb wurde auch in diesem DNS-Szenario dieser ausgewählt. Die
		Credits werden von den Backend-Servern über TCP zum Lastverteiler übermittelt. Die salbd
		Konfiguration für den Lastverteiler (ib1) ist in Anhang \ref{lst:salbd-server} und die
		Konfigurationen für salbd auf den Backend-Servern (ib4, ib6 und ib8) ist in Anhang
		\ref{lst:salbd-client} und \ref{lst:salbd-bind} angeführt.  Jede Messung wurde 51 mal
		wiederholt, da die Messwerte stark schwanken. Somit wurden 306 Messungen durchgeführt

		\begin{table}
			\centering
			\begin{tabular}{|c|cc|}\hline
				Anfragen & 1003149 & \\
				Sessions & 1026 & \\
				\nicefrac{Anfragen}{Sekunde} & 9,33 &\\
				\nicefrac{Anfragen}{Session} & 977,73 &\\
				A & 327861 & \unit[32,68]{\%}\\
				AAAA & 609821 & \unit[60,79]{\%}\\
				NS & 3575 & \unit[0,36]{\%}\\
				MX & 669 & \unit[0,07]{\%}\\
				PTR & 60553 & \unit[6,04]{\%} \\ \hline
			\end{tabular}
			\caption{Analyse des BIND Logs der \texttt{haiti.cs.uni-potsdam.de}-Domain}
			\label{tab:log}
		\end{table}

		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{plots/requests}
			\caption{Anfragen pro Sekunden für das Log vom 29. September 2011 von 5:55 bis 6:00}
			\label{fig:requests}
		\end{figure}

		\begin{table}
			\centering
			\begin{tabular}{|lrrrr|}\hline
				Faktor & Anfragen & Sessions & $\varnothing$ \nicefrac{Anfragen}{Sekunde} &
				maximale \nicefrac{Anfragen}{Sekunde} \\\hline\hline
				1 & 22.594 & 33 & 75,31 & 204 \\
				400 & 9.037.600 & 13.200 & 30.125,33 & 81.600 \\
				800 & 18.075.200 & 26.400 & 60.250,67 & 163.200 \\
				1600 & 36.150.400 & 52.800 & 120.501,33 & 326.400 \\\hline
			\end{tabular}
			\caption{Kennzahlen des modifizierten Log Intervalls}
			\label{tab:multiply}
		\end{table}


		% section Messplan }}}

		\section{Auswertung} % {{{
		\label{sec:auswertung}

		Um die Messungen auszuwerten wurden während der Messungen minütlich die Werte der aktuellen CPU
		und Memory-Auslastung sowie der Load-Wert der Maschinene ib1, ib4, ib6 und ib8 abgefragt. Dies
		wurde mit Hilfe von \texttt{net-snmp}\ durchgeführt. Außerdem wurden die Ausgaben vom servload
		Benchmark gespeichert. 
			
		% section Auswertung }}}

		\section{Fazit} % {{{
		\label{sec:messungen-fazit}
		
		% section Fazit }}}

	% chapter Messungen }}}

	\chapter{Zusammenfassung und Ausblick} % {{{
	\label{cha:zusammenfassung}

		\section{Zusammenfassung} % {{{
		\label{sec:zusammenfassung}
			
		% section Zusammenfassung }}}

		\section{Ausblick} % {{{
		\label{sec:ausblick}

		Metrik funktioniert, aber:
		\begin{itemize}
			\item Ist sie zu optimistisch?
			\item vielleicht eher abschätzung über Größe des bis jetzt größten Pakets
			\item Kann man diese Kernel spezifische Magic allgemeiner machen
			\item Ist es den aufwand Wert den exakten Verlauf zwischen 2 Meldungen zu betrachten und ein
				Tendenz mit in die Berechnung der Credits mit einfließen zu lassen
		\end{itemize}
		
		% section Ausblick }}}

	% chapter Zusammenfassung }}}

	% Mainmatter }}}

	% Appendix {{{ 
	\appendix

	\chapter{Messumgebung} % {{{ 
	\label{cha:messumgebung}

	\lstinputlisting[language=sh,caption={salbd Konfiguration für ib1	(LVS)},label=lst:salbd-server]{listings/salbd.conf.server}

	\lstinputlisting[language=sh,caption={salbd Konfiguration für ib4, ib6 und ib8 (BIND)},label=lst:salbd-client]{listings/salbd.conf.client}

	\lstinputlisting[language=sh,caption={salbd BIND Konfiguration für ib4, ib6 und	ib8},label=lst:salbd-bind]{listings/salbd.networks.conf.client}

	\lstinputlisting[language=sh,caption={BIND Konfiguration für ib4, ib6 und ib8},label=lst:bind-conf]{listings/named.conf}

	\lstinputlisting[language=,breaklines=true,numbers=none,caption={UnixBench Resultate für die Maschinen ib4, ib6 und
	ib8},label=lst:unixbench]{listings/result_unixbench.dat}

	% chapter Messumgebung }}}

	\chapter{sk\_buff} % {{{
	\label{cha:skbuff}

	\lstinputlisting[language=C,firstline=193,lastline=304,caption={\texttt{sk\_buff}-Datenstruktur in der Datei
	\texttt{skbuff.h} vom Linux-Kernel 2.6.18},label=lst:skbuff]{listings/skbuff.h}

	% chapter skbuff }}}

	% Appendix }}}

	% Backmatter {{{
	\backmatter
	\pagenumbering{Roman}

	\listoffigures{}
	\listoftables{}

	% Bibliography {{{
	% \nocite{*}
	\bibliographystyle{dinat}
	\bibliography{references}
	% }}}

	% Backmatter }}}

\end{document}
